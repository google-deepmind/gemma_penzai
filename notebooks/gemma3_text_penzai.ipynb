{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KRvI3fKkNHm"
      },
      "source": [
        "# Testing Gemma3 text-only models in Penzai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Vb78LwkSt7"
      },
      "source": [
        "This colab shows how to load and conduct model forward of Gemma3 text-only using\n",
        "our new package `gemma_penzai`. The original Penzai only supports Gemma1 and\n",
        "Gemma2 models. The current version extends such support. Additionally, we extend\n",
        "the decoding methods with top-p and top-k sampling.\n",
        "\n",
        "NOTE: we run this colab on a TPU **v5e-1** runtime. Please see our notebook\n",
        "`./notebooks/gemma3_multimodal_penzai.ipynb` on how to build a local runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBhKCZ6_lKQu"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmoct3Zp3oLy"
      },
      "source": [
        "Firstly, we install `jax[tpu]`, `gemma_penzai` package and its dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Dp3IfXkf0MO"
      },
      "outputs": [],
      "source": [
        "# Clone the gemma_penzai package\n",
        "!git clone https://github.com/google-deepmind/gemma_penzai.git\n",
        "\n",
        "# Upgrade your pip in case\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# Installs JAX with TPU support\n",
        "!pip install -U \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "\n",
        "# Install the package in editable mode (-e)\n",
        "# This installs dependencies defined in your pyproject.toml\n",
        "print(\"Installing gemma_penzai and dependencies...\")\n",
        "%cd gemma_penzai\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHStPEYcnrEU"
      },
      "source": [
        "Import miscellaneous packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iwAPVcfpIPE"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "from gemma import gm\n",
        "from IPython.display import clear_output\n",
        "import kagglehub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UATXjEzKSwRK"
      },
      "source": [
        "Import JAX related packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWGFCkNvSuOk"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import orbax.checkpoint\n",
        "\n",
        "# check whether connects to TPU\n",
        "jax.devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNncRWjKS4Eb"
      },
      "source": [
        "Import `penzai` related packages (NOTE: we use the most up-to-dated version)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPoY8ETBlP8h"
      },
      "outputs": [],
      "source": [
        "from penzai import pz\n",
        "from penzai.toolshed import jit_wrapper\n",
        "from penzai.toolshed import token_visualization\n",
        "import treescope\n",
        "\n",
        "treescope.basic_interactive_setup(autovisualize_arrays=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW3m2dhbTTWb"
      },
      "source": [
        "Import `gemma_penzai` package to use Gemma3 models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yL8AJVsQTaKz"
      },
      "outputs": [],
      "source": [
        "from gemma_penzai import mllm\n",
        "\n",
        "gemma_from_pretrained_checkpoint = (\n",
        "    mllm.load_gemma.gemma_from_pretrained_checkpoint\n",
        ")\n",
        "sampling_mode = mllm.sampling_mode\n",
        "simple_decoding_loop = mllm.simple_decoding_loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xm9AuB3r_9Q"
      },
      "source": [
        "## Load Gemma3 models from Penzai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpYYkiyETT6x"
      },
      "source": [
        "### Load model parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGev0x3MseYi"
      },
      "source": [
        "You can download the Gemma checkpoints using a Kaggle account and an API key. If\n",
        "you don't have an API key already, you can:\n",
        "\n",
        "1.  Visit https://www.kaggle.com/ and create an account if needed.\n",
        "\n",
        "2.  Go to your account settings, then the 'API' section.\n",
        "\n",
        "3.  Click 'Create new token' to download your key.\n",
        "\n",
        "Next, input your \"KAGGLE_USERNAME\" and \"KAGGLE_KEY\" below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXNukMN6rN4Y"
      },
      "outputs": [],
      "source": [
        "KAGGLE_USERNAME = \"<KAGGLE_USERNAME>\"\n",
        "KAGGLE_KEY = \"<KAGGLE_KEY>\"\n",
        "try:\n",
        "  kagglehub.config.set_kaggle_credentials(KAGGLE_USERNAME, KAGGLE_KEY)\n",
        "except ImportError:\n",
        "  kagglehub.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnDl3c1c3_xq"
      },
      "source": [
        "We load Gemma3-4B instruction model. The checkpoint path could be found in\n",
        "[Gemma's Documentation](https://gemma-llm.readthedocs.io/en/latest/checkpoints.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRRV9EKsswbY"
      },
      "outputs": [],
      "source": [
        "weights_dir = kagglehub.model_download(\"google/gemma-3/flax/gemma3-4b-it\")\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK8m2xkMvWmJ"
      },
      "outputs": [],
      "source": [
        "ckpt_path = os.path.join(weights_dir, \"gemma3-4b-it\")\n",
        "checkpointer = orbax.checkpoint.PyTreeCheckpointer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a18wC0cQsqF0"
      },
      "source": [
        "Here, we don't split model parameters. Optionally, we can shard parameters into\n",
        "different devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haoNry5UsqbM"
      },
      "outputs": [],
      "source": [
        "flat_params = checkpointer.restore(ckpt_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vkq6qjpTYhm"
      },
      "source": [
        "### Bind with Penzai model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBM4QCyptLij"
      },
      "source": [
        "Now we prepare the Gemma3 language model definition and bind it with the\n",
        "parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXaHHAK2tEsV"
      },
      "outputs": [],
      "source": [
        "model = gemma_from_pretrained_checkpoint(\n",
        "    flat_params,\n",
        "    upcast_activations_to_float32=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFgpE0mgTbs6"
      },
      "source": [
        "### Model visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNmoVsn7v_bZ"
      },
      "source": [
        "Directly visualizing the model definition with parameters will take a long time.\n",
        "Therefore, we firstly use `unbind_params` function to extract the model\n",
        "architecture. Then we only visualize the model architecture without parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50NCLe2_v8dV"
      },
      "outputs": [],
      "source": [
        "model_unbound, _ = pz.unbind_params(model)\n",
        "model_unbound"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWy1KtNEcrLX"
      },
      "source": [
        "Free some memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNlRL3Ljcq1Y"
      },
      "outputs": [],
      "source": [
        "del flat_params\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P9BQzRwTqEM"
      },
      "source": [
        "## Text generation for Gemma3 models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77_dBQP0Tukz"
      },
      "source": [
        "### Prepare the inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLJOSmaVl46l"
      },
      "source": [
        "We directly use Gemma3 tokenizer to prepare the input. Here is the tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEbNAPmLluhd"
      },
      "outputs": [],
      "source": [
        "tokenizer = gm.text.Gemma3Tokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD4G7Tl0l5mg"
      },
      "source": [
        "The total number of tokens is available through .vocab_size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljh497M5l0r0"
      },
      "outputs": [],
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOU6jgpe8VjU"
      },
      "source": [
        "As we utilize an instruction model, we prepare our prompt in a multi-turn style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mm-4LD6a8U0D"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"<start_of_turn>user\n",
        "Share one methapore linking \"shadow\" and \"laughter\".<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OkxE8k_nLOe"
      },
      "source": [
        "Then we tokenize the prompt into tokens. Please note that `add_bos=True` should\n",
        "be explicitly passed as Gemma3 without <bos> cannot work normally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bnftk0IGnOYy"
      },
      "outputs": [],
      "source": [
        "token_ids = tokenizer.encode(prompt, add_bos=True)\n",
        "token_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm4iCFven4PF"
      },
      "source": [
        "Then we transform token_ids to named jax arrays with named axes `batch` and\n",
        "`seq`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc54nb7HwcZv"
      },
      "outputs": [],
      "source": [
        "tokens = jnp.asarray(token_ids)[None, :]\n",
        "tokens = pz.nx.wrap(tokens).tag(\"batch\", \"seq\")\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16kBPXK3xAjI"
      },
      "source": [
        "We can also use `token_visualization` in Penzai to visualize the input token\n",
        "ids. Please note that `show_token_array` needs an argument of `SentencePiece`\n",
        "object. To achieve this, we can pass `tokenizer._sp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4calHH6wfA4"
      },
      "outputs": [],
      "source": [
        "token_visualization.show_token_array(tokens, tokenizer._sp)  # pylint: disable=protected-access"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t34qKYzbT2M5"
      },
      "source": [
        "### Test model forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qamDG059xKb"
      },
      "source": [
        "Check model forward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xWc05bg9zc7"
      },
      "outputs": [],
      "source": [
        "logits = model(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eNAaHeE99Ox"
      },
      "source": [
        "We take the last token logits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHg9FyeU97sk"
      },
      "outputs": [],
      "source": [
        "last_logits_penzai = logits.untag(\"seq\")[-1]\n",
        "last_logits_penzai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD2TDKQET44G"
      },
      "source": [
        "### Prepare the model with KV cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdS15un3zWU1"
      },
      "source": [
        "Before the inference, we first prepare an inference mode by adding KV cache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICeuGWHvxoV-"
      },
      "outputs": [],
      "source": [
        "inference_model = sampling_mode.KVCachingTransformerLM.from_uncached(\n",
        "    model,\n",
        "    cache_len=1024,\n",
        "    batch_axes={\"batch\": 1},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXihVQp3UB-R"
      },
      "source": [
        "### Greedy sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQe6OkG5zrJc"
      },
      "source": [
        "Then we jit the model and sample the output from the loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kf6gnpUxzx3X"
      },
      "outputs": [],
      "source": [
        "samples = simple_decoding_loop.temperature_sample_pyloop(\n",
        "    (\n",
        "        pz.select(inference_model)\n",
        "        .at(lambda root: root.body)\n",
        "        .apply(jit_wrapper.Jitted)\n",
        "    ),\n",
        "    prompt=tokens,\n",
        "    temperature=0.0,\n",
        "    rng=jax.random.key(3),\n",
        "    max_sampling_steps=512,\n",
        ")\n",
        "sample_tokens = samples.untag(\"batch\", \"seq\").unwrap()[0]\n",
        "penzai_out1 = tokenizer.decode(sample_tokens)\n",
        "penzai_out1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXXAwQuKUIN9"
      },
      "source": [
        "### Random sampling with temperature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un9yQtG84-UF"
      },
      "source": [
        "Random sampling with temperature 0.8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQXlxAQs1woE"
      },
      "outputs": [],
      "source": [
        "inference_model = sampling_mode.KVCachingTransformerLM.from_uncached(\n",
        "    model,\n",
        "    cache_len=1024,\n",
        "    batch_axes={\"batch\": 1},\n",
        ")\n",
        "samples = simple_decoding_loop.temperature_sample_pyloop(\n",
        "    (\n",
        "        pz.select(inference_model)\n",
        "        .at(lambda root: root.body)\n",
        "        .apply(jit_wrapper.Jitted)\n",
        "    ),\n",
        "    prompt=tokens,\n",
        "    temperature=0.8,\n",
        "    rng=jax.random.key(3),\n",
        "    max_sampling_steps=512,\n",
        ")\n",
        "sample_tokens = samples.untag(\"batch\", \"seq\").unwrap()[0]\n",
        "penzai_out2 = tokenizer.decode(sample_tokens)\n",
        "penzai_out2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhtpmT2kUKuo"
      },
      "source": [
        "### Top-p Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2O5_vLL5CHv"
      },
      "source": [
        "Top_p sampling with temperature 1.0 and top_p 0.95."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQEIGBo_4mGf"
      },
      "outputs": [],
      "source": [
        "inference_model = sampling_mode.KVCachingTransformerLM.from_uncached(\n",
        "    model,\n",
        "    cache_len=1024,\n",
        "    batch_axes={\"batch\": 1},\n",
        ")\n",
        "samples = simple_decoding_loop.temperature_sample_pyloop(\n",
        "    (\n",
        "        pz.select(inference_model)\n",
        "        .at(lambda root: root.body)\n",
        "        .apply(jit_wrapper.Jitted)\n",
        "    ),\n",
        "    prompt=tokens,\n",
        "    temperature=1.0,\n",
        "    top_p=0.95,\n",
        "    rng=jax.random.key(3),\n",
        "    max_sampling_steps=512,\n",
        ")\n",
        "sample_tokens = samples.untag(\"batch\", \"seq\").unwrap()[0]\n",
        "penzai_out3 = tokenizer.decode(sample_tokens)\n",
        "penzai_out3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvoE8qirUNIl"
      },
      "source": [
        "### Top-k sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXn6weni5Iu1"
      },
      "source": [
        "Top_k sampling with temperature 1.0 and top_k 20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBEGeABO4m8s"
      },
      "outputs": [],
      "source": [
        "inference_model = sampling_mode.KVCachingTransformerLM.from_uncached(\n",
        "    model,\n",
        "    cache_len=1024,\n",
        "    batch_axes={\"batch\": 1},\n",
        ")\n",
        "samples = simple_decoding_loop.temperature_sample_pyloop(\n",
        "    (\n",
        "        pz.select(inference_model)\n",
        "        .at(lambda root: root.body)\n",
        "        .apply(jit_wrapper.Jitted)\n",
        "    ),\n",
        "    prompt=tokens,\n",
        "    temperature=1.0,\n",
        "    top_k=20,\n",
        "    rng=jax.random.key(3),\n",
        "    max_sampling_steps=512,\n",
        ")\n",
        "sample_tokens = samples.untag(\"batch\", \"seq\").unwrap()[0]\n",
        "penzai_out4 = tokenizer.decode(sample_tokens)\n",
        "penzai_out4"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
