{
  "cells": [
    {
      "metadata": {
        "id": "2gEILVBEOYmW"
      },
      "cell_type": "markdown",
      "source": [
        "# Gemma Scope 2 Tutorial with Gemma_Penzai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This colab shows how to use\n",
        "[Gemma Scope 2](https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/)\n",
        "in gemma_penzai. Gemma Scope 2 is Google DeepMind's open suite of\n",
        "interpretability tools for all\n",
        "[Gemma 3](https://deepmind.google/models/gemma/gemma-3/) model sizes, from 270 M\n",
        "to 27B parameters. It combines sparse autoencoders (SAEs) and transcoders (TCs),\n",
        "which allows for looking inside LLMs.\n",
        "\n",
        "SAEs are an interpretability tool that act like a \"microscope\" on language model\n",
        "activations to find individual concepts. TCs expand this by finding circuits\n",
        "connecting concepts together.\n",
        "\n",
        "We aim to reproduce main examples in the\n",
        "[Gemma Scope 2 Tutorial](https://colab.sandbox.google.com/drive/1NhWjg7n0nhfW--CjtsOdw5A5J_-Bzn4r?usp=sharing).\n",
        "Please also check our tutorial on Gemma Scope 1 with penzai in the same folder\n",
        "if you are interested.\n",
        "\n",
        "NOTE: we run this colab on a TPU **v5e-1** runtime. Please see our notebook\n",
        "`./notebooks/gemma3_multimodal_penzai.ipynb` on how to build a local runtime."
      ],
      "metadata": {
        "id": "N8mRgCRKQE0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import packages"
      ],
      "metadata": {
        "id": "W7FTVLz9SEE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, we install `jax[tpu]`, `gemma_penzai` package and its dependencies."
      ],
      "metadata": {
        "id": "hy89fOc_SJp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the gemma_penzai package\n",
        "!git clone https://github.com/google-deepmind/gemma_penzai.git\n",
        "\n",
        "# Upgrade your pip in case\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# Installs JAX with TPU support\n",
        "!pip install -U \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "\n",
        "# Install the package in editable mode (-e)\n",
        "# This installs dependencies defined in your pyproject.toml\n",
        "print(\"Installing gemma_penzai and dependencies...\")\n",
        "%cd gemma_penzai\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "5gDZkj9WSB9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import miscellaneous packages."
      ],
      "metadata": {
        "id": "SYgRC2EzSQFX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_h2ZT56OYJQ"
      },
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "import os\n",
        "from gemma import gm\n",
        "from huggingface_hub import hf_hub_download\n",
        "import kagglehub\n",
        "import numpy as np\n",
        "from safetensors.torch import load_file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import plot and display utilities."
      ],
      "metadata": {
        "id": "RFtZ-JUb5ofE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "from IPython.display import display\n",
        "from IPython.display import HTML\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.io as pio"
      ],
      "metadata": {
        "id": "r6BDhnvY5mgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import JAX related packages."
      ],
      "metadata": {
        "id": "09LBApPxS_hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import orbax.checkpoint"
      ],
      "metadata": {
        "id": "I63wuUepS-3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Penzai related packages."
      ],
      "metadata": {
        "id": "DUiH-PwsTDHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import penzai\n",
        "from penzai import pz\n",
        "from penzai.toolshed import jit_wrapper\n",
        "import treescope\n",
        "\n",
        "treescope.basic_interactive_setup(autovisualize_arrays=True)"
      ],
      "metadata": {
        "id": "JJsBeIw7SVR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import `gemma_penzai` package to use Gemma3 models."
      ],
      "metadata": {
        "id": "53GHzi4wTGSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gemma_penzai import mllm\n",
        "\n",
        "gemma_from_pretrained_checkpoint = (\n",
        "    mllm.load_gemma.gemma_from_pretrained_checkpoint\n",
        ")\n",
        "sampling_mode = mllm.sampling_mode\n",
        "simple_decoding_loop = mllm.simple_decoding_loop"
      ],
      "metadata": {
        "id": "Xko-32ovTF6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Gemma 3 models"
      ],
      "metadata": {
        "id": "Q_cuGgpTSauh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can download the Gemma checkpoints using a Kaggle account and an API key. If\n",
        "you don't have an API key already, you can:\n",
        "\n",
        "1.  Visit https://www.kaggle.com/ and create an account if needed.\n",
        "\n",
        "2.  Go to your account settings, then the 'API' section.\n",
        "\n",
        "3.  Click 'Create new token' to download your key.\n",
        "\n",
        "Next, input your \"KAGGLE_USERNAME\" and \"KAGGLE_KEY\" below."
      ],
      "metadata": {
        "id": "G0lRCD25wNK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KAGGLE_USERNAME = \"<KAGGLE_USERNAME>\"\n",
        "KAGGLE_KEY = \"<KAGGLE_KEY>\"\n",
        "try:\n",
        "  kagglehub.config.set_kaggle_credentials(KAGGLE_USERNAME, KAGGLE_KEY)\n",
        "except ImportError:\n",
        "  kagglehub.login()"
      ],
      "metadata": {
        "id": "UFoM9SdUwPci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we load Gemma 3 1B pre-trained model, the second smallest model that Gemma\n",
        "Scope 2 works for (you can also try Gemma 3 270m, but in a Colab the 1B-size\n",
        "model should work fine)."
      ],
      "metadata": {
        "id": "9dOTFokaSeog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights_dir = kagglehub.model_download(\"google/gemma-3/flax/gemma3-1b\")\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "G5Ay-ilNSdmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_path = os.path.join(weights_dir, \"gemma3-1b\")\n",
        "checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
        "metadata = checkpointer.metadata(ckpt_path)"
      ],
      "metadata": {
        "id": "4O6Ij-crwj4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we don't split model parameters. Optionally, we can shard parameters into\n",
        "different devices."
      ],
      "metadata": {
        "id": "AJnvCF33TeHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flat_params = checkpointer.restore(ckpt_path)"
      ],
      "metadata": {
        "id": "ubiRCaEiTdiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we prepare the Gemma3 language model definition and bind it with the\n",
        "parameters. Note here we upcast the activation precision to float32 by passing\n",
        "`upcast_activations_to_float32=True` (default is `False`)."
      ],
      "metadata": {
        "id": "jVDJIbFWTlv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = gemma_from_pretrained_checkpoint(\n",
        "    flat_params,\n",
        "    upcast_activations_to_float32=True,\n",
        ")"
      ],
      "metadata": {
        "id": "IILdfrgqTblI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Directly visualizing the model definition with parameters will take a long time.\n",
        "Therefore, we firstly use `unbind_params` function to extract the model\n",
        "architecture. Then we only visualize the model architecture without parameters."
      ],
      "metadata": {
        "id": "8RsULmmPTpco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_unbound, _ = pz.unbind_params(model)\n",
        "model_unbound"
      ],
      "metadata": {
        "id": "6s9tfaQWTrmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've loaded the model, let's try running it! Before that, let's prepare our\n",
        "tokenizer."
      ],
      "metadata": {
        "id": "j2xeCOKlTxhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = gm.text.Gemma3Tokenizer()"
      ],
      "metadata": {
        "id": "CEDVUW8mUALg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's give it a prompt and tokenize it."
      ],
      "metadata": {
        "id": "8SCK5QBZUCRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The input text\n",
        "prompt_physics = (\n",
        "    \"The law of conservation of energy states that energy cannot be created or\"\n",
        "    \" destroyed, only transformed.\"\n",
        ")\n",
        "\n",
        "token_ids = tokenizer.encode(prompt_physics, add_bos=True)\n",
        "tokens = jnp.asarray(token_ids)[None, :]\n",
        "tokens = pz.nx.wrap(tokens).tag(\"batch\", \"seq\")\n",
        "tokens"
      ],
      "metadata": {
        "id": "YyCe7fg2Tr5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before the inference, we first prepare an inference mode by adding KV cache. And\n",
        "then we jit the model and sample the output from the loop."
      ],
      "metadata": {
        "id": "5kWinNctWnWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_model = sampling_mode.KVCachingTransformerLM.from_uncached(\n",
        "    model,\n",
        "    cache_len=1024,\n",
        "    batch_axes={\"batch\": 1},\n",
        ")\n",
        "samples = simple_decoding_loop.temperature_sample_pyloop(\n",
        "    (\n",
        "        pz.select(inference_model)\n",
        "        .at(lambda root: root.body)\n",
        "        .apply(jit_wrapper.Jitted)\n",
        "    ),\n",
        "    prompt=tokens,\n",
        "    temperature=1.0,\n",
        "    top_p=0.95,\n",
        "    # top_k=64,\n",
        "    rng=jax.random.key(3),\n",
        "    max_sampling_steps=50,\n",
        ")\n",
        "sample_tokens = samples.untag(\"batch\", \"seq\").unwrap()[0]\n",
        "penzai_out = tokenizer.decode(sample_tokens)\n",
        "penzai_out"
      ],
      "metadata": {
        "id": "FiGM80y0UqiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This was the pretrained (PT) model, so it doesn't respond like a chatbot - it\n",
        "just continues based on its priors for what is likely to follow the initial\n",
        "prompt, given the dataset it was trained on.\n",
        "\n",
        "We'll also be using the instruction-tuned (IT) model, which behaves more like a\n",
        "standard chatbot.Let's also load that in and see how it works. Note that we have\n",
        "to carefully format the input so that it's in the correct form for our IT model:"
      ],
      "metadata": {
        "id": "E0kZQsFyVsUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights_dir_it = kagglehub.model_download(\"google/gemma-3/flax/gemma3-1b-it\")\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "lTZFOu3NxPtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_path_it = os.path.join(weights_dir_it, \"gemma3-1b-it\")\n",
        "checkpointer_it = orbax.checkpoint.PyTreeCheckpointer()\n",
        "flat_params_it = checkpointer.restore(ckpt_path_it)\n",
        "model_it = gemma_from_pretrained_checkpoint(\n",
        "    flat_params_it,\n",
        "    upcast_activations_to_float32=True,\n",
        ")"
      ],
      "metadata": {
        "id": "xR6pYDvrxeat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(input_prompt: str) -> str:\n",
        "  return f\"\"\"<start_of_turn>user\n",
        "{input_prompt}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# prepare prompt and tokenize it\n",
        "user_prompt = \"What is your name?\"\n",
        "it_inputs = tokenizer.encode(\n",
        "    format_prompt(input_prompt=user_prompt), add_bos=True\n",
        ")\n",
        "it_tokens = jnp.asarray(it_inputs)[None, :]\n",
        "it_tokens = pz.nx.wrap(it_tokens).tag(\"batch\", \"seq\")\n",
        "\n",
        "\n",
        "# prepare inference mode and generate output\n",
        "inference_model_it = sampling_mode.KVCachingTransformerLM.from_uncached(\n",
        "    model_it,\n",
        "    cache_len=1024,\n",
        "    batch_axes={\"batch\": 1},\n",
        ")\n",
        "samples_it = simple_decoding_loop.temperature_sample_pyloop(\n",
        "    (\n",
        "        pz.select(inference_model_it)\n",
        "        .at(lambda root: root.body)\n",
        "        .apply(jit_wrapper.Jitted)\n",
        "    ),\n",
        "    prompt=it_tokens,\n",
        "    temperature=1.0,\n",
        "    top_p=0.95,\n",
        "    rng=jax.random.key(3),\n",
        "    max_sampling_steps=50,\n",
        ")\n",
        "sample_tokens_it = samples_it.untag(\"batch\", \"seq\").unwrap()[0]\n",
        "penzai_out_it = tokenizer.decode(sample_tokens_it)\n",
        "penzai_out_it = penzai_out_it.split(\"<end_of_turn>\")[0]\n",
        "penzai_out_it"
      ],
      "metadata": {
        "id": "8YtlnTG9WL9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sparse Autoencoders"
      ],
      "metadata": {
        "id": "wj2DnWkUW_nl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's load one of our SAEs. GemmaScope actually contains over four hundred\n",
        "SAEs, but for now we'll just load one on the residual stream at the end of layer\n",
        "22 (of 26, note that layers start at 0 so this is the 23rd layer. This is a\n",
        "fairly late layer, so the model should have time to find more abstract\n",
        "concepts!).\n",
        "\n",
        "The specific filename can be found at\n",
        "[google/gemma-scope-2](https://huggingface.co/collections/google/gemma-scope-2)\n",
        "in `huggingface`."
      ],
      "metadata": {
        "id": "UVelXum4qRYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = 22  # @param [7, 13, 17, 22]\n",
        "width = \"65k\"  # @param [\"16k\", \"65k\", \"262k\", \"1m\"]\n",
        "l0 = \"medium\"  # @param [\"small\", \"medium\", \"big\"]\n",
        "\n",
        "path_to_params = hf_hub_download(\n",
        "    repo_id=\"google/gemma-scope-2-1b-pt\",\n",
        "    filename=(\n",
        "        f\"resid_post/layer_{layer}_width_{width}_l0_{l0}/params.safetensors\"\n",
        "    ),\n",
        ")\n",
        "params = load_file(path_to_params)\n",
        "params = {k: v.numpy() for k, v in params.items()}\n",
        "params"
      ],
      "metadata": {
        "id": "aWSZWybvmRNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the dimensions for SAE parameters."
      ],
      "metadata": {
        "id": "OgfNGbgh5YX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sae_params = {k: v.shape for k, v in params.items()}\n",
        "sae_params"
      ],
      "metadata": {
        "id": "yuH5x7Il5cPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing the SAE"
      ],
      "metadata": {
        "id": "O0oify3CAz3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define the forward pass of the SAE for pedagogical purposes using Penzai.\n",
        "\n",
        "Gemma Scope 2 is a collection of\n",
        "[JumpReLU SAEs](https://arxiv.org/abs/2407.14435), which is like an auto-encoder\n",
        "with both encoder and decoder. The encoder is defined to map the activations\n",
        "into a sparse, non-negative vector of feature magnitude:\n",
        "\n",
        "$$\\boldsymbol{f}(\\boldsymbol{x})=\\sigma(\\boldsymbol{W}_{\\text{enc}}\\boldsymbol{x}+\\boldsymbol{b}_{\\text{enc}})$$\n",
        "\n",
        "Here $\\sigma$ is **JumpReLU** activation defined as ($H$ is the Heaviside step\n",
        "function and $\\theta$ is the threshold.)\n",
        "\n",
        "$$\\sigma(z)=zH(z-\\theta)$$\n",
        "\n",
        "Then the decoder reconstructs the input activations by:\n",
        "\n",
        "$$\\hat{\\boldsymbol{x}}=\\boldsymbol{W}_{\\text{dec}}\\boldsymbol{f}+\\boldsymbol{b}_{\\text{dec}}$$"
      ],
      "metadata": {
        "id": "FnukfNnKwlw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As Penzai has not implemented such a JumpReLU auto-encoder, we first implement a\n",
        "class of `AutoEncoder` with properties of `encoder` and `decoder`. The model\n",
        "forward also includes `encode()` and `decode()`. Then we implement a class of\n",
        "`JumpReLU` with learnable parameters."
      ],
      "metadata": {
        "id": "ymU9cY0WwvUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any\n",
        "\n",
        "from penzai.core import named_axes\n",
        "from penzai.core import struct\n",
        "from penzai.nn import layer as layer_base\n",
        "from penzai.nn import parameters\n",
        "from penzai.nn.linear_and_affine import LinearOperatorWeightInitializer\n",
        "from penzai.nn.linear_and_affine import zero_initializer\n",
        "\n",
        "\n",
        "NamedArray = named_axes.NamedArray\n",
        "\n",
        "\n",
        "@struct.pytree_dataclass\n",
        "class AutoEncoder(pz.nn.Layer):\n",
        "  \"\"\"Top-level auto-encoder wrapper.\n",
        "\n",
        "  Attributes:\n",
        "    encoder: The encoder to transform inputs to latents.\n",
        "    decoder: The decoder to reconstruct inputs from latents.\n",
        "  \"\"\"\n",
        "\n",
        "  encoder: pz.nn.Layer\n",
        "  decoder: pz.nn.Layer\n",
        "\n",
        "  def __call__(\n",
        "      self, x: named_axes.NamedArray, **side_inputs: Any\n",
        "  ) -> named_axes.NamedArray:\n",
        "    \"\"\"Applies the forward pass of the auto-encoder.\"\"\"\n",
        "    acts = self.encode(x, **side_inputs)\n",
        "    recons = self.decode(acts, **side_inputs)\n",
        "    return recons\n",
        "\n",
        "  def encode(\n",
        "      self, x: named_axes.NamedArray, **side_inputs: Any\n",
        "  ) -> named_axes.NamedArray:\n",
        "    \"\"\"Applies the encoder sublayer.\"\"\"\n",
        "    return self.encoder(x, **side_inputs)\n",
        "\n",
        "  def decode(\n",
        "      self, acts: named_axes.NamedArray, **side_inputs: Any\n",
        "  ) -> named_axes.NamedArray:\n",
        "    \"\"\"Applies the decoder sublayer.\"\"\"\n",
        "    return self.decoder(acts, **side_inputs)\n",
        "\n",
        "\n",
        "@struct.pytree_dataclass\n",
        "class JumpReLU(pz.nn.Layer):\n",
        "  \"\"\"JumpReLU activation.\"\"\"\n",
        "\n",
        "  threshold: parameters.ParameterLike[NamedArray]\n",
        "  new_axis_names: tuple[str, ...] = dataclasses.field(\n",
        "      metadata={\"pytree_node\": False}\n",
        "  )\n",
        "  act_fn: layer_base.Layer = pz.nn.Elementwise(jax.nn.relu)\n",
        "\n",
        "  def __call__(self, value: NamedArray, **_unused_side_inputs) -> NamedArray:\n",
        "    \"\"\"Return whether the value is above the threshold.\"\"\"\n",
        "    # Elementwise functions broadcast automatically\n",
        "    return (value > self.threshold.value) * self.act_fn(value)\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(\n",
        "      cls,\n",
        "      name: str,\n",
        "      init_base_rng: jax.Array | None,\n",
        "      threshold_axes: dict[str, int],\n",
        "      new_output_axes: dict[str, int] | None = None,\n",
        "      initializer: LinearOperatorWeightInitializer = zero_initializer,\n",
        "      dtype: jax.typing.DTypeLike = jnp.float32,\n",
        "  ):\n",
        "    \"\"\"Constructs an ``JumpReLU`` layer from a configuration.\n",
        "\n",
        "    Args:\n",
        "      name: The name of the layer.\n",
        "      init_base_rng: The base RNG to use for initializing model parameters.\n",
        "      threshold_axes: Names and lengths for the axes in the input that the\n",
        "        threshold should act over. Other axes will be broadcast over.\n",
        "      new_output_axes: Names and lengths of new axes that should be introduced\n",
        "        into the input.\n",
        "      initializer: Function to use to initialize the weight. Only the output\n",
        "        axes will be set.\n",
        "      dtype: Dtype for the threshold.\n",
        "\n",
        "    Returns:\n",
        "      A new ``AddThreshold`` layer with an uninitialized threshold parameter.\n",
        "    \"\"\"\n",
        "    if new_output_axes is None:\n",
        "      new_output_axes = {}\n",
        "\n",
        "    return cls(\n",
        "        threshold=parameters.make_parameter(\n",
        "            f\"{name}/threshold\",\n",
        "            init_base_rng,\n",
        "            initializer,\n",
        "            input_axes={},\n",
        "            output_axes={**threshold_axes, **new_output_axes},\n",
        "            parallel_axes={},\n",
        "            convolution_spatial_axes={},\n",
        "            dtype=dtype,\n",
        "        ),\n",
        "        new_axis_names=tuple(new_output_axes.keys()),\n",
        "    )\n",
        "\n",
        "  def treescope_color(self) -> str:\n",
        "    return \"#65cfbc\""
      ],
      "metadata": {
        "id": "6BwZICjJwl_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the definition of the above model layers, we implement the model\n",
        "definition of the whole SAE and bind it with parameters."
      ],
      "metadata": {
        "id": "FPFRz24o1dGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sae_from_gemma_scope2(\n",
        "    params_sae: dict[str, Any],\n",
        ") -> AutoEncoder:\n",
        "  \"\"\"Constructs an SAE model from Gemma scope 2 parameters.\n",
        "\n",
        "  Args:\n",
        "    params_sae: The parameters of the Gemma scope 2.\n",
        "\n",
        "  Returns:\n",
        "    A new SAE model.\n",
        "  \"\"\"\n",
        "  embedding_dim, latents_dim = params_sae[\"w_enc\"].shape\n",
        "\n",
        "  # Encoder\n",
        "  encoder = pz.nn.Sequential([\n",
        "      pz.nn.Linear.from_config(\n",
        "          name=\"sae/w_enc\",\n",
        "          init_base_rng=None,\n",
        "          input_axes={\"embedding\": embedding_dim},\n",
        "          output_axes={\"latents\": latents_dim},\n",
        "      ),\n",
        "      pz.nn.AddBias.from_config(\n",
        "          name=\"sae/b_enc\",\n",
        "          init_base_rng=None,\n",
        "          biased_axes={\"latents\": latents_dim},\n",
        "      ),\n",
        "      JumpReLU.from_config(\n",
        "          name=\"sae\",\n",
        "          init_base_rng=None,\n",
        "          threshold_axes={\"latents\": latents_dim},\n",
        "      ),\n",
        "  ])\n",
        "  # Decoder\n",
        "  decoder = pz.nn.Sequential([\n",
        "      pz.nn.Linear.from_config(\n",
        "          name=\"sae/w_dec\",\n",
        "          init_base_rng=None,\n",
        "          input_axes={\"latents\": latents_dim},\n",
        "          output_axes={\"embedding\": embedding_dim},\n",
        "      ),\n",
        "      pz.nn.AddBias.from_config(\n",
        "          name=\"sae/b_dec\",\n",
        "          init_base_rng=None,\n",
        "          biased_axes={\"embedding\": embedding_dim},\n",
        "      ),\n",
        "  ])\n",
        "\n",
        "  # Create the model definition.\n",
        "  model_def = AutoEncoder(\n",
        "      encoder=encoder,\n",
        "      decoder=decoder,\n",
        "  )\n",
        "\n",
        "  # Create parameter objects for each parameter.\n",
        "  model_params = [\n",
        "      pz.Parameter(\n",
        "          value=pz.nx.wrap(params_sae[\"w_enc\"]).tag(\"embedding\", \"latents\"),\n",
        "          label=\"sae/w_enc.weights\",\n",
        "      ),\n",
        "      pz.Parameter(\n",
        "          value=pz.nx.wrap(params_sae[\"b_enc\"]).tag(\"latents\"),\n",
        "          label=\"sae/b_enc.bias\",\n",
        "      ),\n",
        "      pz.Parameter(\n",
        "          value=pz.nx.wrap(params_sae[\"w_dec\"]).tag(\"latents\", \"embedding\"),\n",
        "          label=\"sae/w_dec.weights\",\n",
        "      ),\n",
        "      pz.Parameter(\n",
        "          value=pz.nx.wrap(params_sae[\"b_dec\"]).tag(\"embedding\"),\n",
        "          label=\"sae/b_dec.bias\",\n",
        "      ),\n",
        "      pz.Parameter(\n",
        "          value=pz.nx.wrap(params_sae[\"threshold\"]).tag(\"latents\"),\n",
        "          label=\"sae/threshold\",\n",
        "      ),\n",
        "  ]\n",
        "\n",
        "  model_sae = pz.bind_variables(\n",
        "      model_def,\n",
        "      model_params,\n",
        "  )\n",
        "  return model_sae"
      ],
      "metadata": {
        "id": "UzQdb1Wg1ceN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By passing the params loaded from huggingface, we now get our SAE model. We can\n",
        "easily visualize the model structure in Penzai."
      ],
      "metadata": {
        "id": "3TnOEur91vMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sae_model = sae_from_gemma_scope2(params)\n",
        "sae_model"
      ],
      "metadata": {
        "id": "LxJtBeHw1u-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the SAE on model activations"
      ],
      "metadata": {
        "id": "sliXRdWxAsPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first get out some activations from the model at the SAE target site.\n",
        "We'll demonstrate how to easily do this 'manually' with gemma_penzai by patching\n",
        "the model forward.\n",
        "\n",
        "In penzai and gemma_penzai, it is easy to insert/delete/change model layers and\n",
        "manipulate activations. The general tutorial is in\n",
        "[Penzai Tutorials](https://penzai.readthedocs.io/en/stable/index.html). Here we\n",
        "only show how to display or save intermediate activations:"
      ],
      "metadata": {
        "id": "CpbpqnCOA8Rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a layer to visualize the middle activations\n",
        "@pz.pytree_dataclass  # <- This tags our class as being a Python dataclass and a JAX pytree node.\n",
        "class DisplayIntermediateValue(\n",
        "    pz.nn.Layer\n",
        "):  # <- pz.nn.Layer is the base class of Penzai layers.\n",
        "\n",
        "  def __call__(self, intermediate_value, **unused_side_inputs) -> Any:\n",
        "    # Show the value:\n",
        "    pz.show(\"Showing an intermediate value:\", intermediate_value)\n",
        "    # And return it unchanged.\n",
        "    return intermediate_value\n",
        "\n",
        "\n",
        "# Define a layer to extract the middle activations\n",
        "@pz.pytree_dataclass\n",
        "class SaveIntermediate(pz.nn.Layer):\n",
        "  saved: pz.StateVariable[Any | None]\n",
        "\n",
        "  def __call__(self, value: Any, /, **_unused_side_inputs) -> Any:\n",
        "    self.saved.value = value\n",
        "    return value"
      ],
      "metadata": {
        "id": "gLdCqDZ0Ao6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a `StateVariable` to save model activations."
      ],
      "metadata": {
        "id": "cEHutvCFF9Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_act = pz.StateVariable(value=None)\n",
        "target_act"
      ],
      "metadata": {
        "id": "xspAr0BtF0OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Penzai, model modifications are generally performed by using `pz.select` to\n",
        "make a modified copy of the original model (but sharing the same parameters).\n",
        "This involves “selecting” the part of the model you want to modify, then\n",
        "applying a modification, similar to the `.at[...].set(...)` syntax for modifying\n",
        "JAX arrays. Here we insert `SaveIntermediate` layer after `layer` st\n",
        "`TransformerBlock`."
      ],
      "metadata": {
        "id": "_FLhWXZZGAx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_patched = (\n",
        "    pz.select(model)\n",
        "    .at_instances_of(penzai.models.transformer.model_parts.TransformerBlock)\n",
        "    .pick_nth_selected(layer)\n",
        "    .insert_after(SaveIntermediate(target_act))\n",
        ")\n",
        "logits = model_patched(tokens)"
      ],
      "metadata": {
        "id": "_Ifw4E5BF_E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can run our SAE on the saved activations."
      ],
      "metadata": {
        "id": "PgBu7vRFGvK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sae_acts = sae_model.encode(\n",
        "    jax.tree_util.tree_map(lambda x: x.astype(jnp.float32), target_act.value)\n",
        ")\n",
        "recon = sae_model.decode(sae_acts)"
      ],
      "metadata": {
        "id": "QEii0uPGGwZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's transform these penzai arrays back to JAX arrays."
      ],
      "metadata": {
        "id": "j7BmOPPjLCcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recon_np = recon.untag(\"batch\", \"seq\", \"embedding\").unwrap()\n",
        "target_act_np = target_act.value.untag(\"batch\", \"seq\", \"embedding\").unwrap()\n",
        "sae_acts_np = sae_acts.untag(\"batch\", \"seq\", \"latents\").unwrap()"
      ],
      "metadata": {
        "id": "7gLCGR4eI9o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's just double check that the model looks sensible by checking that we\n",
        "explain a decent chunk of the variance:"
      ],
      "metadata": {
        "id": "l9XwkQp2G5qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reconstruction_mse = jnp.mean((recon_np[:, 1:] - target_act_np[:, 1:]) ** 2)\n",
        "target_variance = target_act_np[:, 1:].var()\n",
        "\n",
        "fvu = reconstruction_mse / target_variance\n",
        "print(f\"Fraction of variance unexplained: {fvu:.2%}\")"
      ],
      "metadata": {
        "id": "PXgzmbVrG7VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks pretty good!\n",
        "\n",
        "This SAE is supposed to have an L0 of ~60 (size \"medium\"), so let's check that\n",
        "too:"
      ],
      "metadata": {
        "id": "Dy6ANHPmL4JR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l0_per_token = (sae_acts_np > 1).sum(-1)[0]\n",
        "print(l0_per_token.tolist())\n",
        "\n",
        "print(f\"Average L0: {l0_per_token[1:].mean():.2f}\")"
      ],
      "metadata": {
        "id": "27L49htuHRxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the SAEs are *NOT* trained on the BOS token because of so called\n",
        "''attention sink'' and ''massive activations'' phenemenon. The first token's\n",
        "activations are outliers and will mess up SAE training. So they tend to give\n",
        "nonsense when we apply to them to it, and we need to be careful not to do this\n",
        "accidentally! We can see this above: the BOS token is a total outlier in terms\n",
        "of L0!"
      ],
      "metadata": {
        "id": "8svVXRh2LQ3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way we can evaluate our SAE is by looking at the **delta loss**, i.e.\n",
        "how much the model's prediction loss increases when we patch in the SAE's\n",
        "output. To do this we'll set up a new penzai layer:"
      ],
      "metadata": {
        "id": "yw2yS6PHLUNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@pz.pytree_dataclass\n",
        "class SAEIntervention(pz.nn.Layer):\n",
        "  \"\"\"Define a layer to intervene the model forward.\"\"\"\n",
        "\n",
        "  sae: pz.nn.Layer\n",
        "\n",
        "  def __call__(self, value: Any, /, **_unused_side_inputs) -> Any:\n",
        "    # first we get the SAE reconstruction\n",
        "    recons = self.sae(\n",
        "        jax.tree_util.tree_map(lambda x: x.astype(jnp.float32), value)\n",
        "    )\n",
        "    # second we only patch the activations except for the BOS token\n",
        "    value = pz.nx.nmap(jnp.concatenate)(\n",
        "        [value.untag(\"seq\")[:1], recons.untag(\"seq\")[1:]], axis=0\n",
        "    ).tag(\"seq\")\n",
        "    return value"
      ],
      "metadata": {
        "id": "pGJv9nHALREZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first get the clean logits by using the normal model forward."
      ],
      "metadata": {
        "id": "-Xaxqw1VpbGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits_clean = model(tokens)"
      ],
      "metadata": {
        "id": "5MtTskZapSHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we patch model with SAE intervention."
      ],
      "metadata": {
        "id": "LuJBCBnspjO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_intervened = (\n",
        "    pz.select(model)\n",
        "    .at_instances_of(penzai.models.transformer.model_parts.TransformerBlock)\n",
        "    .pick_nth_selected(layer)\n",
        "    .insert_after(SAEIntervention(sae=sae_model))\n",
        ")\n",
        "logits_sae = model_intervened(tokens)"
      ],
      "metadata": {
        "id": "HXEJqApkpeo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afterwards, we compute the cross entropy loss for both clean and SAE logits, and\n",
        "then obtain the delta loss."
      ],
      "metadata": {
        "id": "5T_fHyvjrNTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(model_logits, input_token_seq):\n",
        "  \"\"\"Measures avg cross entropy loss.\"\"\"\n",
        "  log_probs = pz.nx.nmap(jax.nn.log_softmax)(\n",
        "      model_logits.untag(\"vocabulary\")\n",
        "  ).tag(\"vocabulary\")\n",
        "  sliced_preds = log_probs[{\"seq\": pz.slice[:-1]}]\n",
        "  correct_next_token = input_token_seq[{\"seq\": pz.slice[1:]}]\n",
        "\n",
        "  log_prob_of_correct_next = sliced_preds[{\"vocabulary\": correct_next_token}]\n",
        "  return -log_prob_of_correct_next"
      ],
      "metadata": {
        "id": "uhB-H8g7rcg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_clean = cross_entropy_loss(logits_clean, tokens)\n",
        "loss_clean = pz.nx.nmap(jnp.mean)(loss_clean.untag(\"batch\", \"seq\")).unwrap()\n",
        "\n",
        "loss_sae = cross_entropy_loss(logits_sae, tokens)\n",
        "loss_sae = pz.nx.nmap(jnp.mean)(loss_sae.untag(\"batch\", \"seq\")).unwrap()\n",
        "\n",
        "print(f\"Loss (clean): {loss_clean.item():.4f}\")\n",
        "print(f\"Loss (corrupted): {loss_sae.item():.4f}\")\n",
        "print(f\"Delta loss: {(loss_sae - loss_clean).item():.4f}\")"
      ],
      "metadata": {
        "id": "9W1LFjDUthty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the highest activating features on this input text, on each token\n",
        "position:"
      ],
      "metadata": {
        "id": "Z9KyFUX9uVn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_activations = sae_acts_np.max(-1)\n",
        "top_features = sae_acts_np.argmax(-1)\n",
        "print(top_features)"
      ],
      "metadata": {
        "id": "Jzxf1q9PuVD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that a lot of these indices are quite small, relative to the number of\n",
        "features in the SAE (over 200 thousand). This is because the SAE was trained\n",
        "with\n",
        "[**Matryoshka loss**](https://www.lesswrong.com/posts/zbebxYCqsryPALh8C/matryoshka-sparse-autoencoders),\n",
        "which imposes a feature hierarchy: the smaller-indexed features are incentivised\n",
        "to be good at reconstructing the input even when all other features are switched\n",
        "off. This helps avoid problems like **feature absorption**."
      ],
      "metadata": {
        "id": "OKRB_uz3vLVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's find the feature which activates the strongest when averaged over all\n",
        "tokens in the sequence:"
      ],
      "metadata": {
        "id": "BgPS2dHLvVFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_acts, top_latents = jax.lax.top_k(sae_acts_np.squeeze().mean(0), 5)\n",
        "for act, idx in zip(top_acts, top_latents):\n",
        "  print(f\"{act:>6.1f} | {idx}\")"
      ],
      "metadata": {
        "id": "YCENYEBAvRDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Latent 6524 seems to fire strongest. Let's inspect it:"
      ],
      "metadata": {
        "id": "SYs1noM1xuoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_idx = 6524\n",
        "\n",
        "activations = sae_acts_np[0, 1:, feature_idx].tolist()\n",
        "str_toks = tokenizer.split(prompt_physics)\n",
        "\n",
        "\n",
        "def html_activations(toks: list[str], acts: list[float]):\n",
        "  return \"\".join(\n",
        "      f\"\"\"<span style=\"background-color: rgba(255,0,0,{v}); padding: 4px\"\"\"\n",
        "      f\"\"\" 0px;\">{t}</span>\"\"\"\n",
        "      for t, v in zip(\n",
        "          toks,\n",
        "          np.array(acts) / (1e-6 + np.max(acts)),\n",
        "          strict=True,\n",
        "      )\n",
        "  )\n",
        "\n",
        "\n",
        "display(HTML(html_activations(str_toks, activations)))"
      ],
      "metadata": {
        "id": "Im4w8weUxu2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One guess we might have is that this latent fires on concepts related to science\n",
        "or scientific laws. Let's test this out with a few examples:"
      ],
      "metadata": {
        "id": "ab4w2lPO0a32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for prompt in [\n",
        "    \"Gemma Scope 2 is a model release from Google DeepMind\",\n",
        "    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit\",\n",
        "    \"Gravity describes how massive objects attract one another\",\n",
        "    \"A charge accelerating through an electric field experiences a force\",\n",
        "    \"Chemical fuel stores energy in molecular bonds, which is released\",\n",
        "]:\n",
        "\n",
        "  inputs = tokenizer.encode(prompt, add_bos=True)\n",
        "  inputs = jnp.asarray(inputs)[None, :]\n",
        "  inputs = pz.nx.wrap(inputs).tag(\"batch\", \"seq\")\n",
        "  logits = model_patched(inputs)\n",
        "  sae_acts = sae_model.encode(\n",
        "      jax.tree_util.tree_map(lambda x: x.astype(jnp.float32), target_act.value)\n",
        "  )\n",
        "\n",
        "  target_act_np = target_act.value.untag(\"batch\", \"seq\", \"embedding\").unwrap()\n",
        "  sae_acts_np = sae_acts.untag(\"batch\", \"seq\", \"latents\").unwrap()\n",
        "\n",
        "  str_toks = tokenizer.split(prompt)\n",
        "\n",
        "  display(\n",
        "      HTML(html_activations(str_toks, sae_acts_np[0, 1:, feature_idx].tolist()))\n",
        "  )\n",
        "  print()"
      ],
      "metadata": {
        "id": "5N6G01om0bFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, so it doesn't fire on the gravity sentence, but it does fire on both the\n",
        "other physics-related sentences as soon as they start talking about forces,\n",
        "energies or fields. This gives us a more specific idea of the concepts this\n",
        "latent might represent.\n",
        "\n",
        "We can investigate this further by looking at the latent's unembedding, in other\n",
        "words **what words get predicted strongest when this latent fires.**"
      ],
      "metadata": {
        "id": "SdtZ2j7dPCiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_u = model.body.sublayers[\n",
        "    -1\n",
        "].table.embeddings.value  # shape (d_vocab, d_model)\n",
        "norm_weight = model.body.sublayers[-2].sublayers[1].weights.value - 1.0\n",
        "w_u_eff = w_u * norm_weight"
      ],
      "metadata": {
        "id": "kQwJ3f7X7vrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply a `-1.0` adjustment to the LN weights to account for implementation\n",
        "differences between the Gemma RMS norm and Penzai. This ensures our results\n",
        "align with the\n",
        "[Gemma Scope 2 Tutorial](https://colab.sandbox.google.com/drive/1NhWjg7n0nhfW--CjtsOdw5A5J_-Bzn4r?usp=sharing).\n",
        "Note that while this operation is arguably optional, it has a negligible impact\n",
        "on the final results."
      ],
      "metadata": {
        "id": "s6ujkf7D9-LJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_vector = sae_model.decoder.sublayers[0].weights.value[\n",
        "    {\"latents\": pz.slice[feature_idx]}\n",
        "]\n",
        "\n",
        "fire_logits = pz.nx.nmap(jnp.matmul)(\n",
        "    w_u_eff.untag(\"embedding\"), decoder_vector.untag(\"embedding\")\n",
        ")\n",
        "top_activations, top_tokens = pz.nx.nmap(jax.lax.top_k)(\n",
        "    fire_logits.untag(\"vocabulary\"), k=10\n",
        ")\n",
        "\n",
        "for act, tok in zip(top_activations.unwrap(), top_tokens.unwrap()):\n",
        "  print(f\"{act:.4f} | {tokenizer.decode(tok)}\")"
      ],
      "metadata": {
        "id": "FCdtvBFwPmIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Steering Model Output"
      ],
      "metadata": {
        "id": "agJ92EtHROyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we can try **steering with this feature**. This means intervening in the\n",
        "residual stream of the model to add some multiple of this feature's decoder\n",
        "vector, so that we can change the behaviour of the model during generation.\n",
        "\n",
        "You should see that when we steer the model on this \"physical force feature\", it\n",
        "starts talking more about physics (specifically forces like electromagnetism or\n",
        "gravity). Note that steering can often be fragile; it's difficult to choose the\n",
        "intervention layer and steering coefficient in a way that gives the expected\n",
        "behavioural change without also breaking the model's coherence. If you're\n",
        "curious, you can try increasing the `coeff` parameter below and seeing what\n",
        "happens!"
      ],
      "metadata": {
        "id": "G63eBQqERTkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@pz.pytree_dataclass\n",
        "class SteerIntermediate(pz.nn.Layer):\n",
        "  \"\"\"Define a layer to steer the middle activations.\"\"\"\n",
        "\n",
        "  steer_vector: pz.StateVariable\n",
        "  coeff: float\n",
        "\n",
        "  def __call__(self, value: Any, /, **_unused_side_inputs) -> Any:\n",
        "    avg_norm = pz.nx.nmap(jnp.linalg.norm)(\n",
        "        value.untag(\"embedding\"), ord=2, axis=-1\n",
        "    )  # , keepdims=True).tag(\"embedding\")\n",
        "    steer_value = value + self.coeff * avg_norm * self.steer_vector\n",
        "    if (\n",
        "        value.named_shape[\"seq\"] != 1\n",
        "    ):  # in prefilling mode, we don't want to change the first token\n",
        "      steer_value = pz.nx.nmap(jnp.concatenate)(\n",
        "          [value.untag(\"seq\")[:1], steer_value.untag(\"seq\")[1:]], axis=0\n",
        "      ).tag(\"seq\")\n",
        "    return steer_value"
      ],
      "metadata": {
        "id": "ovMfr_ShRSXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can first check the model output without model steering."
      ],
      "metadata": {
        "id": "uHcU094VYVT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare model inputs\n",
        "user_prompt = \"Tell me a fun fact.\"\n",
        "it_inputs = tokenizer.encode(format_prompt(user_prompt), add_bos=True)\n",
        "it_tokens = jnp.asarray(it_inputs)[None, :]\n",
        "it_tokens = pz.nx.wrap(it_tokens).tag(\"batch\", \"seq\")\n",
        "\n",
        "# prepare inference model and then sample the outputs\n",
        "inference_model_it = sampling_mode.KVCachingTransformerLM.from_uncached(\n",
        "    model_it,\n",
        "    cache_len=1024,\n",
        "    batch_axes={\"batch\": 1},\n",
        ")\n",
        "samples = simple_decoding_loop.temperature_sample_pyloop(\n",
        "    (\n",
        "        pz.select(inference_model_it)\n",
        "        .at(lambda root: root.body)\n",
        "        .apply(jit_wrapper.Jitted)\n",
        "    ),\n",
        "    prompt=it_tokens,\n",
        "    temperature=0.0,  # greedy generation\n",
        "    rng=jax.random.key(1),\n",
        "    max_sampling_steps=80,\n",
        ")\n",
        "sample_tokens = samples.untag(\"batch\", \"seq\").unwrap()[0]\n",
        "penzai_out = tokenizer.decode(sample_tokens).split(\"<end_of_turn>\")[0]\n",
        "penzai_out"
      ],
      "metadata": {
        "id": "oqX8d9juYjx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's steer the model output by using our defined `SteerIntermediate` layer."
      ],
      "metadata": {
        "id": "zzdIwKu1UjSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_it_steered = (\n",
        "    pz.select(model_it)\n",
        "    .at_instances_of(penzai.models.transformer.model_parts.TransformerBlock)\n",
        "    .pick_nth_selected(layer - 8)\n",
        "    .insert_after(\n",
        "        SteerIntermediate(\n",
        "            steer_vector=jax.tree_util.tree_map(\n",
        "                lambda x: x.astype(jnp.float32), decoder_vector\n",
        "            ),\n",
        "            coeff=0.14,\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "inference_model_it_steered = sampling_mode.KVCachingTransformerLM.from_uncached(\n",
        "    model_it_steered,\n",
        "    cache_len=1024,\n",
        "    batch_axes={\"batch\": 1},\n",
        ")\n",
        "\n",
        "samples = simple_decoding_loop.temperature_sample_pyloop(\n",
        "    (\n",
        "        pz.select(inference_model_it_steered)\n",
        "        .at(lambda root: root.body)\n",
        "        .apply(jit_wrapper.Jitted)\n",
        "    ),\n",
        "    prompt=it_tokens,\n",
        "    temperature=0.0,  # greedy generation\n",
        "    rng=jax.random.key(1),\n",
        "    max_sampling_steps=80,\n",
        ")\n",
        "sample_tokens = samples.untag(\"batch\", \"seq\").unwrap()[0]\n",
        "penzai_out = tokenizer.decode(sample_tokens).split(\"<end_of_turn>\")[0]\n",
        "penzai_out"
      ],
      "metadata": {
        "id": "fVJdQEv6YFEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that steering is expected to be pretty brittle with smaller models.\n",
        "Generally, larger models (up to a certain point) can better express more complex\n",
        "concepts and are easier to steer without breaking coherence.\n",
        "\n",
        "As an exercise, try finding more latents to steer with. Can you come up with any\n",
        "other interesting prompts and latents?"
      ],
      "metadata": {
        "id": "yxTrVL7pUyal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcoders"
      ],
      "metadata": {
        "id": "oENeiaMGf348"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **transcoder** is very similar to an SAE, except rather than reconstructing an\n",
        "activation vector, it reconstructs the mapping from input vector to some output\n",
        "(commonly the input and output of an MLP layer). In this way, rather than\n",
        "decomposing a model's **representations**, it decomposes a model's\n",
        "**computations**.\n",
        "\n",
        "Note - this is where a new weight is introduced, the **affine skip connection**.\n",
        "This is a learned linear transformation from the input to output activations of\n",
        "the transcoder. You can view it as the learned linear component of the MLP\n",
        "layer, and the latents represent the nonlinear components."
      ],
      "metadata": {
        "id": "-c_yM6wuvrLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = 17  # @param [7, 13, 17, 22]\n",
        "width = \"65k\"  # @param [\"16k\", \"65k\", \"262k\", \"1m\"]\n",
        "l0 = \"medium\"  # @param [\"small\", \"medium\", \"big\"]\n",
        "\n",
        "path_to_params = hf_hub_download(\n",
        "    repo_id=\"google/gemma-scope-2-1b-pt\",\n",
        "    filename=f\"transcoder/layer_{layer}_width_{width}_l0_{l0}_affine/params.safetensors\",\n",
        ")\n",
        "params = load_file(path_to_params)\n",
        "params = {k: v.numpy() for k, v in params.items()}"
      ],
      "metadata": {
        "id": "oWKLUhTlf2Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcoder_params = {k: v.shape for k, v in params.items()}\n",
        "transcoder_params"
      ],
      "metadata": {
        "id": "QBajdYUQ3O44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing the Transcoder"
      ],
      "metadata": {
        "id": "2V4H2tCh-OyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transcoder uses affine residual connection for the encoder input. The encoder is\n",
        "the same:\n",
        "\n",
        "$$\\boldsymbol{f}(\\boldsymbol{x})=\\sigma(\\boldsymbol{W}_{\\text{enc}}\\boldsymbol{x}+\\boldsymbol{b}_{\\text{enc}})$$\n",
        "\n",
        "Here $\\sigma$ is **JumpReLU** activation defined as ($H$ is the Heaviside step\n",
        "function and $\\theta$ is the threshold.)\n",
        "\n",
        "Then the decoder reconstructs the input activations by:\n",
        "\n",
        "$$\\hat{\\boldsymbol{x}}=\\boldsymbol{W}_{\\text{dec}}\\boldsymbol{f}+\\boldsymbol{b}_{\\text{dec}} + \\boldsymbol{W}_{\\text{skip}}\\boldsymbol{x}$$"
      ],
      "metadata": {
        "id": "8OcK7AoC0VIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@struct.pytree_dataclass\n",
        "class AutoEncoderSkip(pz.nn.Layer):\n",
        "  \"\"\"Top-level auto-encoder with skip connection wrapper.\n",
        "\n",
        "  Attributes:\n",
        "    encoder: The encoder to transform inputs to latents.\n",
        "    decoder: The decoder to reconstruct inputs from latents.\n",
        "  \"\"\"\n",
        "\n",
        "  encoder: pz.nn.Layer\n",
        "  decoder: pz.nn.Layer\n",
        "\n",
        "  def __call__(\n",
        "      self, x: named_axes.NamedArray, **side_inputs: Any\n",
        "  ) -> named_axes.NamedArray:\n",
        "    \"\"\"Applies the forward pass of the auto-encoder.\"\"\"\n",
        "    acts = self.encode(x, **side_inputs)\n",
        "    recons = self.decode(acts, x, **side_inputs)\n",
        "    return recons\n",
        "\n",
        "  def encode(\n",
        "      self, x: named_axes.NamedArray, **side_inputs: Any\n",
        "  ) -> named_axes.NamedArray:\n",
        "    \"\"\"Applies the encoder sublayer.\"\"\"\n",
        "    return self.encoder(x, **side_inputs)\n",
        "\n",
        "  def decode(\n",
        "      self,\n",
        "      acts: named_axes.NamedArray,\n",
        "      x: named_axes.NamedArray,\n",
        "      **side_inputs: Any,\n",
        "  ) -> named_axes.NamedArray:\n",
        "    \"\"\"Applies the decoder sublayer.\"\"\"\n",
        "    return self.decoder(acts, x, **side_inputs)\n",
        "\n",
        "\n",
        "@struct.pytree_dataclass\n",
        "class JoinTwoBranch(pz.nn.Layer):\n",
        "  \"\"\"A joiner to combine multiple branches with individual inputs.\n",
        "\n",
        "  Attributes:\n",
        "    branch1: The branch network to handle input 1.\n",
        "    branch2: The branch network to handle input 2.\n",
        "  \"\"\"\n",
        "\n",
        "  branch1: pz.nn.Layer\n",
        "  branch2: pz.nn.Layer\n",
        "\n",
        "  def __call__(\n",
        "      self,\n",
        "      x: named_axes.NamedArray,\n",
        "      y: named_axes.NamedArray,\n",
        "      **side_inputs: Any,\n",
        "  ) -> named_axes.NamedArray:\n",
        "    \"\"\"Applies the forward pass of the joiner.\"\"\"\n",
        "    x = self.branch1(x, **side_inputs)\n",
        "    y = self.branch2(y, **side_inputs)\n",
        "    return x + y"
      ],
      "metadata": {
        "id": "kcy4KYsF7X71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transcoder_from_gemma_scope2(\n",
        "    params_transcoder: dict[str, Any],\n",
        ") -> AutoEncoder:\n",
        "  \"\"\"Constructs an Transcoder model from Gemma scope 2 parameters.\n",
        "\n",
        "  Args:\n",
        "    params_transcoder: The parameters of the Gemma scope 2.\n",
        "\n",
        "  Returns:\n",
        "    A new SAE model.\n",
        "  \"\"\"\n",
        "  embedding_dim, latents_dim = params_transcoder[\"w_enc\"].shape\n",
        "\n",
        "  # Encoder\n",
        "  encoder = pz.nn.Sequential([\n",
        "      pz.nn.Linear.from_config(\n",
        "          name=\"transcoder/w_enc\",\n",
        "          init_base_rng=None,\n",
        "          input_axes={\"embedding\": embedding_dim},\n",
        "          output_axes={\"latents\": latents_dim},\n",
        "      ),\n",
        "      pz.nn.AddBias.from_config(\n",
        "          name=\"transcoder/b_enc\",\n",
        "          init_base_rng=None,\n",
        "          biased_axes={\"latents\": latents_dim},\n",
        "      ),\n",
        "      JumpReLU.from_config(\n",
        "          name=\"transcoder\",\n",
        "          init_base_rng=None,\n",
        "          threshold_axes={\"latents\": latents_dim},\n",
        "      ),\n",
        "  ])\n",
        "  # Decoder\n",
        "\n",
        "  decoder_branch1_layers = pz.nn.Sequential([\n",
        "      pz.nn.Linear.from_config(\n",
        "          name=\"transcoder/w_dec\",\n",
        "          init_base_rng=None,\n",
        "          input_axes={\"latents\": latents_dim},\n",
        "          output_axes={\"embedding\": embedding_dim},\n",
        "      ),\n",
        "      pz.nn.AddBias.from_config(\n",
        "          name=\"transcoder/b_dec\",\n",
        "          init_base_rng=None,\n",
        "          biased_axes={\"embedding\": embedding_dim},\n",
        "      ),\n",
        "  ])\n",
        "  decoder_branch2_layers = pz.nn.Sequential([\n",
        "      pz.nn.Linear.from_config(\n",
        "          name=\"transcoder/affine_skip_connection\",\n",
        "          init_base_rng=None,\n",
        "          input_axes={\"embedding\": embedding_dim},\n",
        "          output_axes={\"new_embedding\": embedding_dim},\n",
        "      ),\n",
        "      pz.nn.RenameAxes(old=\"new_embedding\", new=\"embedding\"),\n",
        "  ])\n",
        "\n",
        "  decoder = JoinTwoBranch(\n",
        "      branch1=decoder_branch1_layers,\n",
        "      branch2=decoder_branch2_layers,\n",
        "  )\n",
        "\n",
        "  # Create the model definition.\n",
        "  model_def = AutoEncoderSkip(\n",
        "      encoder=encoder,\n",
        "      decoder=decoder,\n",
        "  )\n",
        "\n",
        "  # Create parameter objects for each parameter.\n",
        "  model_params = [\n",
        "      pz.Parameter(\n",
        "          value=pz.nx.wrap(params_transcoder[\"w_enc\"]).tag(\n",
        "              \"embedding\", \"latents\"\n",
        "          ),\n",
        "          label=\"transcoder/w_enc.weights\",\n",
        "      ),\n",
        "      pz.Parameter(\n",
        "          value=pz.nx.wrap(params_transcoder[\"b_enc\"]).tag(\"latents\"),\n",
        "          label=\"transcoder/b_enc.bias\",\n",
        "      ),\n",
        "      pz.Parameter(\n",
        "          value=pz.nx.wrap(params_transcoder[\"w_dec\"]).tag(\n",
        "              \"latents\", \"embedding\"\n",
        "          ),\n",
        "          label=\"transcoder/w_dec.weights\",\n",
        "      ),\n",
        "      pz.Parameter(\n",
        "          value=pz.nx.wrap(params_transcoder[\"b_dec\"]).tag(\"embedding\"),\n",
        "          label=\"transcoder/b_dec.bias\",\n",
        "      ),\n",
        "      pz.Parameter(\n",
        "          value=pz.nx.wrap(params_transcoder[\"threshold\"]).tag(\"latents\"),\n",
        "          label=\"transcoder/threshold\",\n",
        "      ),\n",
        "      pz.Parameter(\n",
        "          value=pz.nx.wrap(params_transcoder[\"affine_skip_connection\"]).tag(\n",
        "              \"embedding\", \"new_embedding\"\n",
        "          ),\n",
        "          label=\"transcoder/affine_skip_connection.weights\",\n",
        "      ),\n",
        "  ]\n",
        "\n",
        "  model_transcoder = pz.bind_variables(\n",
        "      model_def,\n",
        "      model_params,\n",
        "  )\n",
        "  return model_transcoder"
      ],
      "metadata": {
        "id": "2xXRK_wl4HZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we bind the transcoder parameters with the model architecture."
      ],
      "metadata": {
        "id": "radrO9Ex-XWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcoder = transcoder_from_gemma_scope2(params)\n",
        "transcoder"
      ],
      "metadata": {
        "id": "p_CJn7WI0hTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the Transcoder on model activations"
      ],
      "metadata": {
        "id": "-AOUDKHjFlKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once nice property about transcoders is that you can use them to find\n",
        "**circuits**. This is because (if you freeze attention patterns) we can model\n",
        "the relationship between two transcoder latents in different layers as being\n",
        "totally **linear**.\n",
        "\n",
        "The input to transcoder is the input to FFN (after layer-norm) and the target of\n",
        "transcoder is the output of FFN (also after layer-norm). Now we patch the model\n",
        "to find these two latents by identifying the location of layer-norms (each\n",
        "transformer block has 6 layer-norm, 4 in attention, 2 in FNN)."
      ],
      "metadata": {
        "id": "rkL-YY89VQ55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre_ffn = pz.StateVariable(value=None)\n",
        "post_ffn = pz.StateVariable(value=None)\n",
        "model_patched = (\n",
        "    pz.select(model)\n",
        "    .at_instances_of(pz.nn.RMSLayerNorm)\n",
        "    .pick_nth_selected(\n",
        "        layer * 6 + 4\n",
        "    )  # each transformer block has 6 layer norm, 4 in attention, 2 in ffn\n",
        "    .insert_after(SaveIntermediate(pre_ffn))\n",
        ")\n",
        "model_patched = (\n",
        "    pz.select(model_patched)\n",
        "    .at_instances_of(pz.nn.RMSLayerNorm)\n",
        "    .pick_nth_selected(layer * 6 + 5)\n",
        "    .insert_after(SaveIntermediate(post_ffn))\n",
        ")"
      ],
      "metadata": {
        "id": "ceUtlcwcBPFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The quick brown fox jumped over the lazy dog\"\n",
        "\n",
        "token_ids = tokenizer.encode(prompt, add_bos=True)\n",
        "tokens = jnp.asarray(token_ids)[None, :]\n",
        "tokens = pz.nx.wrap(tokens).tag(\"batch\", \"seq\")\n",
        "logits = model_patched(tokens)"
      ],
      "metadata": {
        "id": "e-sm2fL0Jb57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sae_acts = transcoder.encode(pre_ffn.value)\n",
        "recon = transcoder.decode(sae_acts, pre_ffn.value)"
      ],
      "metadata": {
        "id": "TjyBsV-bFXCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recon_np = recon.untag(\"batch\", \"seq\", \"embedding\").unwrap()\n",
        "sae_target_np = post_ffn.value.untag(\"batch\", \"seq\", \"embedding\").unwrap()\n",
        "sae_acts_np = sae_acts.untag(\"batch\", \"seq\", \"latents\").unwrap()"
      ],
      "metadata": {
        "id": "lxn7Hy7UFvR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = jnp.mean((recon_np[:, 1:] - sae_target_np[:, 1:]) ** 2)\n",
        "var = sae_target_np[:, 1:].var()\n",
        "fvu = mse / var\n",
        "l0 = (sae_acts_np[:, 1:] > 0).sum(-1).mean()\n",
        "\n",
        "print(f\"L0: {l0:.2f}\")\n",
        "print(f\"Fraction of variance unexplained: {mse / var:.2%}\")"
      ],
      "metadata": {
        "id": "3LlBNWWhGBv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a higher FVU than our sparse autoencoder. But since the output of a\n",
        "single MLP is a less important causal node than the residual stream (which\n",
        "contains **all accumulated information** up to that layer in the model),\n",
        "transcoders usually have a smaller delta loss when we patch in their output.\n",
        "\n",
        "Let's test this, with a slight modification of our previous patching function:"
      ],
      "metadata": {
        "id": "YmJt6bDjPlNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a layer to intervene the model forward\n",
        "@pz.pytree_dataclass\n",
        "class TranscoderIntervention(pz.nn.Layer):\n",
        "  transcoder_recon: named_axes.NamedArray\n",
        "\n",
        "  def __call__(self, value: Any, /, **_unused_side_inputs) -> Any:\n",
        "    # we only patch the activations except for the BOS token\n",
        "    value = pz.nx.nmap(jnp.concatenate)(\n",
        "        [value.untag(\"seq\")[:1], self.transcoder_recon.untag(\"seq\")[1:]], axis=0\n",
        "    ).tag(\"seq\")\n",
        "    return value"
      ],
      "metadata": {
        "id": "LwU8eKq3PkxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_physics = (\n",
        "    \"The law of conservation of energy states that energy cannot be created or\"\n",
        "    \" destroyed, only transformed.\"\n",
        ")\n",
        "token_ids = tokenizer.encode(prompt_physics, add_bos=True)\n",
        "tokens = jnp.asarray(token_ids)[None, :]\n",
        "tokens = pz.nx.wrap(tokens).tag(\"batch\", \"seq\")\n",
        "\n",
        "logits_clean = model_patched(tokens)\n",
        "sae_acts = transcoder.encode(pre_ffn.value)\n",
        "recon = transcoder.decode(sae_acts, pre_ffn.value)"
      ],
      "metadata": {
        "id": "-n61XCLAXJdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_intervened = (\n",
        "    pz.select(model)\n",
        "    .at_instances_of(pz.nn.RMSLayerNorm)\n",
        "    .pick_nth_selected(layer * 6 + 5)\n",
        "    .insert_after(TranscoderIntervention(recon))\n",
        ")\n",
        "logits_sae = model_intervened(tokens)"
      ],
      "metadata": {
        "id": "q4vXeuZJQ0Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_clean = (\n",
        "    cross_entropy_loss(logits_clean, tokens).untag(\"batch\", \"seq\").unwrap()[0]\n",
        ")\n",
        "\n",
        "loss_sae = (\n",
        "    cross_entropy_loss(logits_sae, tokens).untag(\"batch\", \"seq\").unwrap()[0]\n",
        ")\n",
        "\n",
        "print(f\"Loss (clean): {loss_clean.mean().item():.4f}\")\n",
        "print(f\"Loss (corrupted): {loss_sae.mean().item():.4f}\")\n",
        "print(f\"Delta loss: {(loss_sae.mean() - loss_clean.mean()).item():.4f}\")"
      ],
      "metadata": {
        "id": "PCDaKUSPRDer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pio.renderers.default = \"colab\"  # Force the renderer to Colab\n",
        "\n",
        "data = {\n",
        "    \"token\": list(range(len(str_toks))),\n",
        "    \"Clean\": loss_clean.tolist(),\n",
        "    \"SAE\": loss_sae.tolist(),\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "px.line(\n",
        "    df, x=\"token\", y=[\"Clean\", \"SAE\"], labels={\"value\": \"Loss\"}\n",
        ").update_layout(\n",
        "    xaxis=dict(tickvals=df[\"token\"], ticktext=str_toks, tickangle=45),\n",
        "    title=\"Cross-entropy loss with SAE intervention\",\n",
        "    width=1000,\n",
        "    height=600,\n",
        ").show()"
      ],
      "metadata": {
        "id": "_tB9pw_65OYh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1i8soHmmJhEZ69_uiQ5B0pdWihJfYjJSJ",
          "timestamp": 1768188143538
        }
      ],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}