{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF8jqpRH_vJV"
      },
      "source": [
        "# Attention sink and Logit lens in Penzai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72EwfTWf_2of"
      },
      "source": [
        "In the first part, this colab shows to use Penzai to visualize/understand\n",
        "attention sink and its related phenemenon, such as massive activations and\n",
        "value-state drains. It involves the literature,\n",
        "[[1](https://arxiv.org/abs/2309.17453), [2](https://arxiv.org/abs/2410.10781),\n",
        "[3](https://arxiv.org/abs/2402.17762), [4](https://arxiv.org/abs/2410.13835),\n",
        "[5](https://arxiv.org/abs/2504.02732)]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz6XIp4jOn-q"
      },
      "source": [
        "In the second part, this colab demonstrates how to use logit lens to analyze\n",
        "model predictions using Penzai. We refer to the blog\n",
        "[interpreting GPT: the logit lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens).\n",
        "Instead of using GPT, we use Gemma models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: we run this colab on a TPU **v5e-1** runtime. Please see our notebook\n",
        "`./notebooks/gemma3_multimodal_penzai.ipynb` on how to build a local runtime."
      ],
      "metadata": {
        "id": "EkkRN-IvRv-6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTjL6CTdPBU6"
      },
      "source": [
        "# First part: Attention sink and its related phenemenon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmATwBbkAAe0"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DD2jg2wXNk0"
      },
      "source": [
        "Firstly, we install `jax[tpu]`, `gemma_penzai` package and its dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3ss3f6NAANh"
      },
      "outputs": [],
      "source": [
        "# Clone the gemma_penzai package\n",
        "!git clone https://github.com/google-deepmind/gemma_penzai.git\n",
        "\n",
        "# Upgrade your pip in case\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# Installs JAX with TPU support\n",
        "!pip install -U \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "\n",
        "# Install the package in editable mode (-e)\n",
        "# This installs dependencies defined in your pyproject.toml\n",
        "print(\"Installing gemma_penzai and dependencies...\")\n",
        "%cd gemma_penzai\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kemy4bsNXTch"
      },
      "source": [
        "Import miscellaneous packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh-kvFhW_2Td"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "from typing import Any\n",
        "from gemma import gm\n",
        "from IPython.display import clear_output\n",
        "import kagglehub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drV_dofUXT_r"
      },
      "source": [
        "Import JAX related packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFmX7XddXQx8"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "from jax.experimental import mesh_utils\n",
        "import jax.numpy as jnp\n",
        "from jax.sharding import Mesh\n",
        "from jax.sharding import NamedSharding\n",
        "from jax.sharding import PartitionSpec\n",
        "import orbax.checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4KkIprZXW58"
      },
      "source": [
        "Import Penzai related packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0auYu25f_jVh"
      },
      "outputs": [],
      "source": [
        "import penzai\n",
        "from penzai import pz\n",
        "from penzai.models import transformer\n",
        "from penzai.toolshed import token_visualization\n",
        "import treescope\n",
        "\n",
        "treescope.basic_interactive_setup(autovisualize_arrays=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXO7mZG0ARmS"
      },
      "source": [
        "## Loading Gemma2 pre-trained models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6Q-GN8ZDGfz"
      },
      "source": [
        "You can download the Gemma checkpoints using a Kaggle account and an API key. If\n",
        "you don't have an API key already, you can:\n",
        "\n",
        "1.  Visit https://www.kaggle.com/ and create an account if needed.\n",
        "\n",
        "2.  Go to your account settings, then the 'API' section.\n",
        "\n",
        "3.  Click 'Create new token' to download your key.\n",
        "\n",
        "Next, input your \"KAGGLE_USERNAME\" and \"KAGGLE_KEY\" below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "KAGGLE_USERNAME = \"<KAGGLE_USERNAME>\"\n",
        "KAGGLE_KEY = \"<KAGGLE_KEY>\"\n",
        "try:\n",
        "  kagglehub.config.set_kaggle_credentials(KAGGLE_USERNAME, KAGGLE_KEY)\n",
        "except ImportError:\n",
        "  kagglehub.login()"
      ],
      "metadata": {
        "id": "W7irfnuSNgty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, we load metadata and checkpoint of Gemma2 2B model."
      ],
      "metadata": {
        "id": "belYi2-mOJEm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alpKrlJwARGk"
      },
      "outputs": [],
      "source": [
        "weights_dir = kagglehub.model_download(\"google/gemma-2/flax/gemma2-2b\")\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_path = os.path.join(weights_dir, \"gemma2-2b\")\n",
        "checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
        "metadata = checkpointer.metadata(ckpt_path)"
      ],
      "metadata": {
        "id": "TPUUIYBHOmSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW5m6139Ac_E"
      },
      "source": [
        "Prepare the device and sharding. Here the sharding strategy splits the model\n",
        "parameters into different TPUs according to the last dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBPf2N0ZaDEJ"
      },
      "outputs": [],
      "source": [
        "n_devices = jax.local_device_count()\n",
        "sharding_devices = mesh_utils.create_device_mesh((n_devices,))\n",
        "mesh = Mesh(sharding_devices, (\"data\",))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT_VmD3LAepo"
      },
      "outputs": [],
      "source": [
        "restore_args = jax.tree_util.tree_map(\n",
        "    lambda m: orbax.checkpoint.ArrayRestoreArgs(\n",
        "        restore_type=jax.Array,\n",
        "        sharding=NamedSharding(\n",
        "            mesh, PartitionSpec(*(None,) * (len(m.shape) - 1), \"data\")\n",
        "        ),\n",
        "    ),\n",
        "    metadata.item_metadata,  # change back to metadata if any running error\n",
        ")\n",
        "flat_params = checkpointer.restore(ckpt_path, restore_args=restore_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeicYvWKAhID"
      },
      "source": [
        "Now we prepare the Gemma2 language model definition and bind it with the\n",
        "parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_3-uSswAfve"
      },
      "outputs": [],
      "source": [
        "model = transformer.variants.gemma.gemma_from_pretrained_checkpoint(\n",
        "    flat_params,\n",
        "    upcast_activations_to_float32=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7k9V4sdBFfm"
      },
      "source": [
        "Directly visualizing the model definition with parameters will take a long time.\n",
        "Therefore, we firstly use `unbind_params` function to extract the model\n",
        "architecture. Then we only visualize the model architecture without parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nU26Q8l-DcKf"
      },
      "outputs": [],
      "source": [
        "model_unbound, _ = pz.unbind_params(model)\n",
        "model_unbound"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3Vuq58dAlQd"
      },
      "source": [
        "Free some memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MveUMn-JAkP4"
      },
      "outputs": [],
      "source": [
        "del flat_params\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu0KIoTHCr_E"
      },
      "source": [
        "## Prepare prompt and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHXkeLGPCw39"
      },
      "outputs": [],
      "source": [
        "tokenizer = gm.text.Gemma2Tokenizer()\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C44qnQdnFjdn"
      },
      "source": [
        "Check the vocubulary size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-L7-ij1Ffv7"
      },
      "outputs": [],
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rVlkoyFFnJL"
      },
      "source": [
        "Check the special token set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIKmBSizFloN"
      },
      "outputs": [],
      "source": [
        "tokenizer.special_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYDgfitrD7OT"
      },
      "source": [
        "Prepare a simple prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVglcxQ3G0uN"
      },
      "outputs": [],
      "source": [
        "prompt = \"Would you be able to travel through time using a wormhole?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UPn7ydyGxim"
      },
      "source": [
        "Use tokenizer to encode the prompt, and then transform it into a named JAX\n",
        "array. Please note that we need to enable `add_bos=True` to ensure Gemma2 models\n",
        "work normally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iiQ0Y51G-u9"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer.encode(prompt, add_bos=True)\n",
        "tokens = jnp.asarray(tokens)[None, :]\n",
        "tokens = pz.nx.wrap(tokens).tag(\"batch\", \"seq\")\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmH-NqSKECO5"
      },
      "source": [
        "We can also use `token_visualization` in Penzai to visualize the input token\n",
        "ids. Please note that `show_token_array` needs an argument of `SentencePiece`\n",
        "object. To achieve this, we can pass `tokenizer._sp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7gyIyHvEB6I"
      },
      "outputs": [],
      "source": [
        "token_visualization.show_token_array(tokens, tokenizer._sp)  # pylint: disable=protected-access"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzsxMe8-OUj8"
      },
      "source": [
        "## Massive activations for the first token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lDQiKwkEIdx"
      },
      "source": [
        "We first use Penzai to visualize the\n",
        "[massive activations](https://arxiv.org/abs/2402.17762), which refers to that\n",
        "the first token has activation outliers compared to other tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7j7gn1cHnGD"
      },
      "source": [
        "In Penzai, it is easy to insert/delete/change model layers and manipulate\n",
        "activations. The general tutorial is in\n",
        "[Penzai Tutorials](https://penzai.readthedocs.io/en/stable/index.html). Here we\n",
        "only show how to display or save intermediate activations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35daJnkhHDky"
      },
      "outputs": [],
      "source": [
        "# Define a layer to visualize the middle activations\n",
        "@pz.pytree_dataclass  # <- This tags our class as being a Python dataclass and a JAX pytree node.\n",
        "class DisplayIntermediateValue(\n",
        "    pz.nn.Layer\n",
        "):  # <- pz.nn.Layer is the base class of Penzai layers.\n",
        "\n",
        "  def __call__(self, intermediate_value, **unused_side_inputs) -> Any:\n",
        "    # Show the value:\n",
        "    pz.show(\"Showing an intermediate value:\", intermediate_value)\n",
        "    # And return it unchanged.\n",
        "    return intermediate_value\n",
        "\n",
        "\n",
        "# Define a layer to extract the middle activations\n",
        "@pz.pytree_dataclass\n",
        "class SaveIntermediate(pz.nn.Layer):\n",
        "  saved: pz.StateVariable[Any | None]\n",
        "\n",
        "  def __call__(self, value: Any, /, **_unused_side_inputs) -> Any:\n",
        "    self.saved.value = value\n",
        "    return value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWlWqwOQH0Oa"
      },
      "source": [
        "In Penzai, model modifications are generally performed by using `pz.select` to\n",
        "make a modified copy of the original model (but sharing the same parameters).\n",
        "This involves “selecting” the part of the model you want to modify, then\n",
        "applying a modification, similar to the `.at[...].set(...)` syntax for modifying\n",
        "JAX arrays.\n",
        "\n",
        "Here we insert an object of `SaveIntermediate` to the model after each\n",
        "transformer block. Remember that when we build a new object of\n",
        "`SaveIntermediate`, a new `StateVariable` needs to be passed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yT5uWBTHsee"
      },
      "outputs": [],
      "source": [
        "block_num = 26\n",
        "model_patched = None\n",
        "all_activations = [pz.StateVariable(value=None) for _ in range(block_num)]\n",
        "for block_index in range(block_num):\n",
        "  if model_patched is None:\n",
        "    model_patched = (\n",
        "        pz.select(model)\n",
        "        .at_instances_of(penzai.models.transformer.model_parts.TransformerBlock)\n",
        "        .pick_nth_selected(block_index)\n",
        "        .insert_after(SaveIntermediate(all_activations[block_index]))\n",
        "    )\n",
        "  else:\n",
        "    model_patched = (\n",
        "        pz.select(model_patched)\n",
        "        .at_instances_of(penzai.models.transformer.model_parts.TransformerBlock)\n",
        "        .pick_nth_selected(block_index)\n",
        "        .insert_after(SaveIntermediate(all_activations[block_index]))\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-3wpVyZIB5N"
      },
      "source": [
        "Then we conduct model forward under patch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi8bJMlEIBZU"
      },
      "outputs": [],
      "source": [
        "logits = model_patched(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYvVdWtYIJYn"
      },
      "source": [
        "Now the residual stream after each transformer block has been saved into the\n",
        "list of `all_activations`. We then visualize the $\\ell_2$-norm for activations\n",
        "and show that the first token has massive activations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDALDcrwIH6g"
      },
      "outputs": [],
      "source": [
        "all_norm = []\n",
        "for block_index in range(block_num):\n",
        "  activations = all_activations[block_index].value\n",
        "  norm = pz.nx.nmap(jnp.linalg.norm)(activations.untag(\"embedding\"), ord=2)\n",
        "  # norm = pz.nx.nmap(jnp.expand_dims)(norm, axis=0).tag(\"block\")\n",
        "  all_norm.append(norm)\n",
        "all_norm = pz.nx.stack(all_norm, axis_name=\"block\")\n",
        "all_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkechxiHFImW"
      },
      "source": [
        "Please put the cursor on each token across different blocks, we can check the\n",
        "$\\ell_2$-norm of each activation. It is clear that the first token is an outlier\n",
        "compared to the other tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC-wtQv2hSo_"
      },
      "source": [
        "## Cosine Similarity among different transformer blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYTVm8fciTkW"
      },
      "source": [
        "We use the same saved activations to analyze how they changed w.r.t. blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ay1mMAHiapH"
      },
      "outputs": [],
      "source": [
        "all_activations_stack = []\n",
        "for block_index in range(block_num):\n",
        "  activations = all_activations[block_index].value\n",
        "  all_activations_stack.append(activations)\n",
        "all_activations_stack = pz.nx.stack(all_activations_stack, axis_name=\"block\")\n",
        "all_activations_stack.named_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YdX-lO8GLnG"
      },
      "source": [
        "Here we measure the cosine-similarity between two consecutive blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cAmmLqQi-Z7"
      },
      "outputs": [],
      "source": [
        "all_activations_stack_prev = all_activations_stack.untag(\"block\")[\n",
        "    : block_num - 1\n",
        "].tag(\"block\")\n",
        "all_activations_stack_next = all_activations_stack.untag(\"block\")[1:].tag(\n",
        "    \"block\"\n",
        ")\n",
        "\n",
        "all_activations_stack_prev_normalized = pz.nx.nmap(\n",
        "    lambda x: x / jnp.linalg.norm(x, ord=2)\n",
        ")(all_activations_stack_prev.untag(\"embedding\"))\n",
        "all_activations_stack_next_normalized = pz.nx.nmap(\n",
        "    lambda x: x / jnp.linalg.norm(x, ord=2)\n",
        ")(all_activations_stack_next.untag(\"embedding\"))\n",
        "\n",
        "cosine_sim = pz.nx.nmap(jnp.dot)(\n",
        "    all_activations_stack_prev_normalized, all_activations_stack_next_normalized\n",
        ")\n",
        "cosine_sim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU7pybY3mgqg"
      },
      "source": [
        "As can be seen here, the cosine similarity between consecutive layers seem to be\n",
        "large, especially for those middle layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Goxq65mIGSZK"
      },
      "outputs": [],
      "source": [
        "cosine_sim > 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu4P5smrOauT"
      },
      "source": [
        "## Attention sink for the first token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K04-dGyMWgyV"
      },
      "source": [
        "Similar to how to extract residual stream from the model, we can construct\n",
        "objects of `SaveIntermediate` and insert them after each attention layer. As we\n",
        "would like to visualize the attention map, we insert them after `pz.nn.Softmax`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dJY-R-kMC8c"
      },
      "outputs": [],
      "source": [
        "block_num = 26\n",
        "model_patched = None\n",
        "attention_maps = [pz.StateVariable(value=None) for _ in range(block_num)]\n",
        "for block_index in range(block_num):\n",
        "  if model_patched is None:\n",
        "    model_patched = (\n",
        "        pz.select(model)\n",
        "        .at_instances_of(pz.nn.Softmax)\n",
        "        .pick_nth_selected(block_index)\n",
        "        .insert_after(SaveIntermediate(attention_maps[block_index]))\n",
        "    )\n",
        "  else:\n",
        "    model_patched = (\n",
        "        pz.select(model_patched)\n",
        "        .at_instances_of(pz.nn.Softmax)\n",
        "        .pick_nth_selected(block_index)\n",
        "        .insert_after(SaveIntermediate(attention_maps[block_index]))\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ-YnnYtHJV4"
      },
      "source": [
        "Then we use the patched model for model forward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EXyZSq9bgtI"
      },
      "outputs": [],
      "source": [
        "logits = model_patched(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFvHx_rLHOAp"
      },
      "source": [
        "As there are many transformer blocks, here we would like to visualize only one\n",
        "block, e.g., the 23rd block (index=22)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxE87_ktbl43"
      },
      "outputs": [],
      "source": [
        "block_index = 22  # @param {type:\"integer\"}\n",
        "attention_maps[block_index].value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCkXqLEWHk85"
      },
      "source": [
        "Here we can clearly see that the first token has extremely large attention\n",
        "scores (close to 0.9 out of 1.0) compared to other tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp1iQx4KdmS1"
      },
      "source": [
        "## Value-state drains for the first token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YsRn7b7IQSc"
      },
      "source": [
        "In the literature of [[2](https://arxiv.org/abs/2410.10781),\n",
        "[4](https://arxiv.org/abs/2410.13835), [5](https://arxiv.org/abs/2504.02732)],\n",
        "the authors mentioned that although the first token has large attention scores,\n",
        "it has small value-states. And the attention operation is the weighted average\n",
        "on the value-states, the first token may make contributions limitedly in\n",
        "semantics, more from the functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvong2OMIy6r"
      },
      "source": [
        "Now we use the same way to extract value states. Penzai provides convenient way\n",
        "to select specific layers. Here we only would like to extract the linear layers\n",
        "related to value states, so we can set a condition by finding the labels\n",
        "including `value.weights`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I1LvwYpdqRE"
      },
      "outputs": [],
      "source": [
        "block_num = 26\n",
        "model_patched = None\n",
        "all_values = [pz.StateVariable(value=None) for _ in range(block_num)]\n",
        "for block_index in range(block_num):\n",
        "  if model_patched is None:\n",
        "    model_patched = (\n",
        "        pz.select(model)\n",
        "        .at_instances_of(pz.nn.Linear)\n",
        "        .where(lambda x: \"attention/value.weights\" in x.weights.label)\n",
        "        .pick_nth_selected(block_index)\n",
        "        .insert_after(SaveIntermediate(all_values[block_index]))\n",
        "    )\n",
        "  else:\n",
        "    model_patched = (\n",
        "        pz.select(model_patched)\n",
        "        .at_instances_of(pz.nn.Linear)\n",
        "        .where(lambda x: \"attention/value.weights\" in x.weights.label)\n",
        "        .pick_nth_selected(block_index)\n",
        "        .insert_after(SaveIntermediate(all_values[block_index]))\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkAw5kRLd-sQ"
      },
      "outputs": [],
      "source": [
        "logits = model_patched(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky86UhUiL3mr"
      },
      "source": [
        "As there are many transformer blocks, here we would like to visualize only one\n",
        "block, e.g., the 15th block (index=14)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IShiwa9OeKCU"
      },
      "outputs": [],
      "source": [
        "block_index = 1  # @param {type:\"integer\"}\n",
        "pz.nx.nmap(jnp.linalg.norm)(\n",
        "    all_values[block_index].value.untag(\"projection\"), ord=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKQgUlHyMFy4"
      },
      "source": [
        "It is observed that the first token has much smaller $\\ell_2$-norm (about 2)\n",
        "compared to that of other tokens (more than 10)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGgyfpR9MXHv"
      },
      "source": [
        "We can also visualize the value states across different heads and transformer\n",
        "blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tItlfNSbsjnt"
      },
      "outputs": [],
      "source": [
        "all_values_norm = []\n",
        "for block_index in range(block_num):\n",
        "  values = all_values[block_index].value\n",
        "  norm = pz.nx.nmap(jnp.linalg.norm)(values.untag(\"projection\"), ord=2)\n",
        "  # norm = pz.nx.nmap(jnp.expand_dims)(norm, axis=0).tag(\"block\")\n",
        "  all_values_norm.append(norm)\n",
        "all_values_norm = pz.nx.stack(all_values_norm, axis_name=\"block\")\n",
        "all_values_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oynwts3Ns4as"
      },
      "source": [
        "We can clearly observe that the $\\ell_2$-norm on the first token is\n",
        "significantly smaller than other tokens.\n",
        "\n",
        "This further shows that attention sink tokens contribute almost no semantic\n",
        "meanings to follow-up tokens. Their existence is due to that the attention\n",
        "scores are normalized to sum up to one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUdoFf8sMxjN"
      },
      "source": [
        "## Disappearance of attention sink without \\<bos\\> token in Gemma models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vEvTORIM6AE"
      },
      "source": [
        "Most of LLMs have attention sink in the first token regardless of \\<bos\\> token\n",
        "according to [2](https://arxiv.org/abs/2410.10781). Gemma models are exceptions,\n",
        "as shown in [5](https://arxiv.org/abs/2504.02732). Without \\<bos\\> token,\n",
        "attention sink disappears. As hypothesized in\n",
        "[5](https://arxiv.org/abs/2504.02732), this is due to the reason that Gemma\n",
        "models always put \\<bos\\> token in the first position during the pre-training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwRaBdPIN--U"
      },
      "source": [
        "Now we encode the same prompt without \\<bos\\> token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHVjzsR6M5nA"
      },
      "outputs": [],
      "source": [
        "tokens_no_bos = tokenizer.encode(prompt, add_bos=False)\n",
        "tokens_no_bos = jnp.asarray(tokens_no_bos)[None, :]\n",
        "tokens_no_bos = pz.nx.wrap(tokens_no_bos).tag(\"batch\", \"seq\")\n",
        "tokens_no_bos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br88tOSHOFad"
      },
      "outputs": [],
      "source": [
        "token_visualization.show_token_array(tokens_no_bos, tokenizer._sp)  # pylint: disable=protected-access"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMdy6-p3OMVC"
      },
      "source": [
        "Now we construct the patched model again and conduct the model forward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahPzWcEROFmQ"
      },
      "outputs": [],
      "source": [
        "block_num = 26\n",
        "model_patched = None\n",
        "attention_maps = [pz.StateVariable(value=None) for _ in range(block_num)]\n",
        "for block_index in range(block_num):\n",
        "  if model_patched is None:\n",
        "    model_patched = (\n",
        "        pz.select(model)\n",
        "        .at_instances_of(pz.nn.Softmax)\n",
        "        .pick_nth_selected(block_index)\n",
        "        .insert_after(SaveIntermediate(attention_maps[block_index]))\n",
        "    )\n",
        "  else:\n",
        "    model_patched = (\n",
        "        pz.select(model_patched)\n",
        "        .at_instances_of(pz.nn.Softmax)\n",
        "        .pick_nth_selected(block_index)\n",
        "        .insert_after(SaveIntermediate(attention_maps[block_index]))\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWFWcmVNOK5r"
      },
      "outputs": [],
      "source": [
        "logits = model_patched(tokens_no_bos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec5cAwN8OUt5"
      },
      "outputs": [],
      "source": [
        "block_index = 22  # @param {type:\"integer\"}\n",
        "attention_maps[block_index].value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAyyDkJJOYKo"
      },
      "source": [
        "As shown here, the attention scores are more distributed in the diagonals\n",
        "instead of the first token. Attention sink phenemenon on the first token\n",
        "disappears."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj1Wa3JIPLOw"
      },
      "source": [
        "Before move to the second part, we can delete the model we use and release some\n",
        "memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DrfPdGWPRVe"
      },
      "outputs": [],
      "source": [
        "del model_patched\n",
        "del model\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GxOzsgBPFMa"
      },
      "source": [
        "# Second part: Logit lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ezg9v7dZPJxi"
      },
      "source": [
        "As the first part already imports the packages, we skip this part. To ensure the\n",
        "diversity of models, we use Gemma3 1B pretrained model instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovENzag9Q3uo"
      },
      "source": [
        "## Load Gemma3 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FthiwYb-Q60H"
      },
      "source": [
        "We load the Gemma3-1B model similarly as above."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint path for Gemma3 model\n",
        "weights_dir = kagglehub.model_download(\"google/gemma-3/flax/gemma3-1b\")\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "8Hw2ocSOQ55-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gHeJWskPJVw"
      },
      "outputs": [],
      "source": [
        "ckpt_path = os.path.join(weights_dir, \"gemma3-1b\")\n",
        "checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
        "metadata = checkpointer.metadata(ckpt_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdJtfUs0ZCVe"
      },
      "outputs": [],
      "source": [
        "n_devices = jax.local_device_count()\n",
        "sharding_devices = mesh_utils.create_device_mesh((n_devices,))\n",
        "mesh = Mesh(sharding_devices, (\"data\",))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJeYBXKNQO-I"
      },
      "outputs": [],
      "source": [
        "restore_args = jax.tree_util.tree_map(\n",
        "    lambda m: orbax.checkpoint.ArrayRestoreArgs(\n",
        "        restore_type=jax.Array,\n",
        "        sharding=NamedSharding(\n",
        "            mesh, PartitionSpec(*(None,) * (len(m.shape) - 1), \"data\")\n",
        "        ),\n",
        "    ),\n",
        "    metadata.item_metadata,\n",
        ")\n",
        "flat_params = checkpointer.restore(ckpt_path, restore_args=restore_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnhCFFKLQRMT"
      },
      "outputs": [],
      "source": [
        "model = transformer.variants.gemma.gemma_from_pretrained_checkpoint(\n",
        "    flat_params,\n",
        "    upcast_activations_to_float32=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPxR28MCQirK"
      },
      "source": [
        "Here we visualize the model definition:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXEe26dkQlo5"
      },
      "outputs": [],
      "source": [
        "model_unbound, _ = pz.unbind_params(model)\n",
        "model_unbound"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvmvMVv1QtdM"
      },
      "source": [
        "Free the memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1vJxvEyQr4V"
      },
      "outputs": [],
      "source": [
        "del flat_params\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TgtPCWDQ7-L"
      },
      "source": [
        "## Prepare the tokenizer and prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9GIL3S0RG3k"
      },
      "source": [
        "For Gemma3 series, we use `Gemma3Tokenizer` instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYeV_MVTQxgm"
      },
      "outputs": [],
      "source": [
        "tokenizer = gm.text.Gemma3Tokenizer()\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8TPtTDURGVF"
      },
      "outputs": [],
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAjpGNHTRMHm"
      },
      "outputs": [],
      "source": [
        "tokenizer.special_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRkJpYaZRREl"
      },
      "source": [
        "As we use a pre-trained model, we prepare a prefix text as the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYLeccI2RNac"
      },
      "outputs": [],
      "source": [
        "prompt = (\n",
        "    \"Specifically, we train GPT-3, an autoregressive language model with 175\"\n",
        "    \" billion parameters,\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bdNrcEzRQqP"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer.encode(prompt, add_bos=True)\n",
        "tokens = jnp.asarray(tokens)\n",
        "tokens = pz.nx.wrap(tokens).tag(\"seq\")\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rV-wDkfqRWwn"
      },
      "outputs": [],
      "source": [
        "token_visualization.show_token_array(tokens, tokenizer._sp)  # pylint: disable=protected-access"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5yZP2-CRbkV"
      },
      "source": [
        "We first check the model forward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ulpc2eYJRYSI"
      },
      "outputs": [],
      "source": [
        "logits = model(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkCkyDDURgHW"
      },
      "outputs": [],
      "source": [
        "pred_tokens = pz.nx.nmap(jnp.argmax)(logits.untag(\"vocabulary\"))\n",
        "token_visualization.show_token_array(pred_tokens, tokenizer._sp)  # pylint: disable=protected-access"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R8QpyT6Rqrh"
      },
      "source": [
        "Note the final output is \"_to\". The previous output may not be sensible as the\n",
        "next token prediction may not be accurate for the first few tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgpyDkEeR7LC"
      },
      "source": [
        "## Logit lens analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKubOjTSSRMd"
      },
      "source": [
        "First, we visualize the model definition again to find the model embedding and\n",
        "unembedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zQQwXUTR1MN"
      },
      "outputs": [],
      "source": [
        "model_unbound"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaIqKgS9SY2X"
      },
      "source": [
        "It is observed that in the transformer architecture, the token ids will first be\n",
        "fed into a layer of `EmbeddingLookup` and then a layer of `ConstantRescale`\n",
        "before input to a series of `TransformerBlock`. To decode the model predictions,\n",
        "there are a final layer of `RMSLayerNorm` and then the LM head\n",
        "`EmbeddingDecode`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nvVgWbMTXs6"
      },
      "source": [
        "The logit lens refers to how logits change after each step of processing. Here\n",
        "the processing mean each transformer block. We can first construct our\n",
        "prediction head, including both the final RMS norm layer and a linear\n",
        "unembedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg_xsuCBSWxa"
      },
      "outputs": [],
      "source": [
        "pred_head = pz.nn.Sequential([\n",
        "    model.body.sublayers[-2],\n",
        "    model.body.sublayers[-1],\n",
        "])\n",
        "pred_head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gEqpaR1T4zi"
      },
      "source": [
        "It is easy to extract model layers, we can directly take the final two layers to\n",
        "construct a new model named as `pred_head`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EYUQYCCUHH2"
      },
      "source": [
        "Then we extract activations after each transformer block. Similar to how we\n",
        "visualize massive activations, we can follow the procedure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVFKtGZmT35u"
      },
      "outputs": [],
      "source": [
        "block_num = 26\n",
        "model_patched = None\n",
        "all_activations = [pz.StateVariable(value=None) for _ in range(block_num)]\n",
        "for block_index in range(block_num):\n",
        "  if model_patched is None:\n",
        "    model_patched = (\n",
        "        pz.select(model)\n",
        "        .at_instances_of(penzai.models.transformer.model_parts.TransformerBlock)\n",
        "        .pick_nth_selected(block_index)\n",
        "        .insert_after(SaveIntermediate(all_activations[block_index]))\n",
        "    )\n",
        "  else:\n",
        "    model_patched = (\n",
        "        pz.select(model_patched)\n",
        "        .at_instances_of(penzai.models.transformer.model_parts.TransformerBlock)\n",
        "        .pick_nth_selected(block_index)\n",
        "        .insert_after(SaveIntermediate(all_activations[block_index]))\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCJzrpViVGNH"
      },
      "outputs": [],
      "source": [
        "logits = model_patched(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0nUPNXDYqII"
      },
      "source": [
        "Now we proceed to probe activations from transformer blocks and observe the\n",
        "dynamics of logits. We use the output of each transformer block as the input to\n",
        "our prediction head, resulting in logits. We then could infer the rank-1 model\n",
        "predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF7TBuywYxBR"
      },
      "outputs": [],
      "source": [
        "all_top_token = []\n",
        "all_top_logit = []\n",
        "for block_index in range(block_num):\n",
        "  activations = all_activations[block_index].value\n",
        "  block_logits = pred_head(activations)\n",
        "  top_token = pz.nx.nmap(jnp.argmax)(block_logits.untag(\"vocabulary\"))\n",
        "  top_logit = pz.nx.nmap(jnp.max)(block_logits.untag(\"vocabulary\"))\n",
        "  all_top_token.append(top_token)\n",
        "  all_top_logit.append(top_logit)\n",
        "all_top_token = pz.nx.stack(all_top_token, axis_name=\"block\")\n",
        "all_top_logit = pz.nx.stack(all_top_logit, axis_name=\"block\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8hX3rGUa7md"
      },
      "source": [
        "Then we visualize the model predictions in the middle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UnU0i60ZBA2"
      },
      "outputs": [],
      "source": [
        "# all_top_token\n",
        "token_visualization.show_token_array(all_top_token, tokenizer._sp)  # pylint: disable=protected-access"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht3VMZr0hnhw"
      },
      "source": [
        "We can observe that the model output in the middle already looks like the final\n",
        "output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wmkaEeFZ_X6"
      },
      "source": [
        "Compute KL-divergence from output distributions $KL(final||layer)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "043uK8aBZ_mZ"
      },
      "outputs": [],
      "source": [
        "all_kl_divergence = []\n",
        "log_probs = pz.nx.nmap(jax.nn.log_softmax)(logits.untag(\"vocabulary\"))\n",
        "probs = pz.nx.nmap(jnp.exp)(log_probs)\n",
        "for block_index in range(block_num):\n",
        "  activations = all_activations[block_index].value\n",
        "  block_logits = pred_head(activations)\n",
        "  block_log_probs = pz.nx.nmap(jax.nn.log_softmax)(\n",
        "      block_logits.untag(\"vocabulary\")\n",
        "  )\n",
        "  kl_divergence = pz.nx.nmap(jnp.sum)(\n",
        "      probs * (log_probs - block_log_probs), axis=-1\n",
        "  )\n",
        "  all_kl_divergence.append(kl_divergence)\n",
        "all_kl_divergence = pz.nx.stack(all_kl_divergence, axis_name=\"block\")\n",
        "all_kl_divergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyAsIYPQhK2L"
      },
      "source": [
        "*   After first few layers, the input has been transformed into something that\n",
        "    looks like the final output.\n",
        "\n",
        "*   After this one discontinuous jump, the distribution progresses in a much\n",
        "    more smooth way to the final output distribution."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [
        {
          "file_id": "1hvbFLMXcTvIaBiQDs9Zpa0lm1uyvgDUm",
          "timestamp": 1767537827996
        }
      ],
      "toc_visible": true,
      "gpuType": "V5E1",
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
