{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrGl1StDoE7h"
      },
      "source": [
        "# Gemma Scope Tutorial with Penzai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlSt6SZVoM5j"
      },
      "source": [
        "This colab shows how to use\n",
        "[Gemma Scope](https://huggingface.co/google/gemma-scope) in Penzai. Gemma Scope\n",
        "is Google DeepMind's suite of Sparse Autoencoders (SAEs) on every layer and\n",
        "sublayer of Gemma2 2B and 9B.\n",
        "\n",
        "Sparse Autoencoders are an interpretability tool that act like a \"microscope\" on\n",
        "language model activations.\n",
        "\n",
        "We aim to reproduce the example in the\n",
        "[Tutorial: Gemma Scope from Scratch](https://colab.sandbox.google.com/drive/17dQFYUYnuKnP6OwQPH9v_GSYUW5aj-Rp?usp=sharing).\n",
        "\n",
        "NOTE: we run this colab on a TPU **v5e-1** runtime. Please see our notebook\n",
        "`./notebooks/gemma3_multimodal_penzai.ipynb` on how to build a local runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8EH5K21_JEi"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4Q1ZoFCeemA"
      },
      "source": [
        "Firstly, we install `jax[tpu]`, `gemma_penzai` package and its dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNRLL8aJoEME"
      },
      "outputs": [],
      "source": [
        "# Clone the gemma_penzai package\n",
        "!git clone https://github.com/google-deepmind/gemma_penzai.git\n",
        "\n",
        "# Upgrade your pip in case\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# Installs JAX with TPU support\n",
        "!pip install -U \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "\n",
        "# Install the package in editable mode (-e)\n",
        "# This installs dependencies defined in your pyproject.toml\n",
        "print(\"Installing gemma_penzai and dependencies...\")\n",
        "%cd gemma_penzai\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlcY-zyoegcB"
      },
      "source": [
        "Import miscellaneous packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwMyNYwLeZtL"
      },
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "import gc\n",
        "import os\n",
        "from gemma import gm\n",
        "from huggingface_hub import hf_hub_download\n",
        "from IPython.display import clear_output\n",
        "import kagglehub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnzPGP6GehaM"
      },
      "source": [
        "Import JAX related packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtG6f0mu-9ZE"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "from jax.experimental import mesh_utils\n",
        "import jax.numpy as jnp\n",
        "from jax.sharding import Mesh\n",
        "from jax.sharding import NamedSharding\n",
        "from jax.sharding import PartitionSpec\n",
        "import numpy as np\n",
        "import orbax.checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU0L3k37ekie"
      },
      "source": [
        "Import Penzai related packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kse9LX_2_BQv"
      },
      "outputs": [],
      "source": [
        "import penzai\n",
        "from penzai import pz\n",
        "from penzai.models import transformer\n",
        "from penzai.toolshed import jit_wrapper\n",
        "from penzai.toolshed import token_visualization\n",
        "import treescope\n",
        "\n",
        "treescope.basic_interactive_setup(autovisualize_arrays=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOs7O3h8_b2c"
      },
      "source": [
        "## Loading Gemma2 pre-trained models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can download the Gemma checkpoints using a Kaggle account and an API key. If\n",
        "you don't have an API key already, you can:\n",
        "\n",
        "1.  Visit https://www.kaggle.com/ and create an account if needed.\n",
        "\n",
        "2.  Go to your account settings, then the 'API' section.\n",
        "\n",
        "3.  Click 'Create new token' to download your key.\n",
        "\n",
        "Next, input your \"KAGGLE_USERNAME\" and \"KAGGLE_KEY\" below."
      ],
      "metadata": {
        "id": "ZOMcy_lYw2Mz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6Dr_Xq3_HZm"
      },
      "outputs": [],
      "source": [
        "KAGGLE_USERNAME = \"<KAGGLE_USERNAME>\"\n",
        "KAGGLE_KEY = \"<KAGGLE_KEY>\"\n",
        "try:\n",
        "  kagglehub.config.set_kaggle_credentials(KAGGLE_USERNAME, KAGGLE_KEY)\n",
        "except ImportError:\n",
        "  kagglehub.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As Gemma Scope is trained on activations of Gemma2, so we first load Gemma2 2B\n",
        "pre-trained models in Penzai."
      ],
      "metadata": {
        "id": "DW7suyT7xGyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights_dir = kagglehub.model_download(\"google/gemma-2/flax/gemma2-2b\")\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "Lx5QneLqxLKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_path = os.path.join(weights_dir, \"gemma2-2b\")\n",
        "checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
        "metadata = checkpointer.metadata(ckpt_path)"
      ],
      "metadata": {
        "id": "AhZ8KyHIxMTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the device and sharding. Here the sharding strategy splits the model\n",
        "parameters into different TPUs according to the last dimension."
      ],
      "metadata": {
        "id": "DwCAJXWAxUt6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvTvXhtXepQg"
      },
      "outputs": [],
      "source": [
        "n_devices = jax.local_device_count()\n",
        "sharding_devices = mesh_utils.create_device_mesh((n_devices,))\n",
        "mesh = Mesh(sharding_devices, (\"data\",))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj2_r7SdAnyT"
      },
      "outputs": [],
      "source": [
        "restore_args = jax.tree_util.tree_map(\n",
        "    lambda m: orbax.checkpoint.ArrayRestoreArgs(\n",
        "        restore_type=jax.Array,\n",
        "        sharding=NamedSharding(\n",
        "            mesh, PartitionSpec(*(None,) * (len(m.shape) - 1), \"data\")\n",
        "        ),\n",
        "    ),\n",
        "    metadata.item_metadata,  # change back to metadata if any running error\n",
        ")\n",
        "flat_params = checkpointer.restore(ckpt_path, restore_args=restore_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNTAdEqEAc25"
      },
      "source": [
        "Now we prepare the Gemma2 language model definition and bind it with the\n",
        "parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M69PQ4STAqiM"
      },
      "outputs": [],
      "source": [
        "model = transformer.variants.gemma.gemma_from_pretrained_checkpoint(\n",
        "    flat_params,\n",
        "    upcast_activations_to_float32=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKMMcuIaXvdO"
      },
      "source": [
        "Directly visualizing the model definition with parameters will take a long time.\n",
        "Therefore, we firstly use `unbind_params` function to extract the model\n",
        "architecture. Then we only visualize the model architecture without parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSfA_JEgQgus"
      },
      "outputs": [],
      "source": [
        "model.body.sublayers[-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo8GXmXaXvMn"
      },
      "outputs": [],
      "source": [
        "model_unbound, _ = pz.unbind_params(model)\n",
        "model_unbound"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceCcN078Aqfw"
      },
      "source": [
        "Free some memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HDhsmS9A3_1"
      },
      "outputs": [],
      "source": [
        "del flat_params\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C96YC18Wo7G"
      },
      "source": [
        "## Evaluate model inference of Gemma2 in Penzai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDSUhDQ7BxSu"
      },
      "source": [
        "Load tokenizer for Gemma2 models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMmuv6mgBwZo"
      },
      "outputs": [],
      "source": [
        "tokenizer = gm.text.Gemma2Tokenizer()\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej_MDnpvYBUj"
      },
      "source": [
        "Show the vocabulary size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx8BlKnKRb0X"
      },
      "outputs": [],
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZBPX2GWYE8n"
      },
      "source": [
        "Show the special tokens in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaSLAsI2R6Ch"
      },
      "outputs": [],
      "source": [
        "tokenizer.special_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEnucuPEYibQ"
      },
      "source": [
        "Use tokenizer to encode the prompt, and then transform it into a named JAX\n",
        "array. Please note that we need to enable `add_bos=True` to ensure Gemma2 models\n",
        "work normally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzXTqr6KCNu3"
      },
      "outputs": [],
      "source": [
        "token_ids = tokenizer.encode(\n",
        "    \"Would you be able to travel through time using a wormhole?\", add_bos=True\n",
        ")\n",
        "tokens = jnp.asarray(token_ids)[None, :]\n",
        "tokens = pz.nx.wrap(tokens).tag(\"batch\", \"seq\")\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne4qpr5aYoQi"
      },
      "source": [
        "We can also use `token_visualization` in Penzai to visualize the input token\n",
        "ids. Please note that `show_token_array` needs an argument of `SentencePiece`\n",
        "object. To achieve this, we can pass `tokenizer._sp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CcHiez7Yquh"
      },
      "outputs": [],
      "source": [
        "token_visualization.show_token_array(tokens, tokenizer._sp)  # pylint: disable=protected-access"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Irm_qZe0YxCK"
      },
      "source": [
        "Before the inference, we first prepare an inference mode by adding KV cache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYJf5vRnCPR0"
      },
      "outputs": [],
      "source": [
        "inference_model = (\n",
        "    transformer.sampling_mode.KVCachingTransformerLM.from_uncached(\n",
        "        model,\n",
        "        cache_len=1024,\n",
        "        batch_axes={\"batch\": 1},\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3BSXe4zY1NX"
      },
      "source": [
        "Then we jit the model and sample the output from the loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcFV8Qrf2Nu8"
      },
      "outputs": [],
      "source": [
        "samples = transformer.simple_decoding_loop.temperature_sample_pyloop(\n",
        "    (\n",
        "        pz.select(inference_model)\n",
        "        .at(lambda root: root.body)\n",
        "        .apply(jit_wrapper.Jitted)\n",
        "    ),\n",
        "    prompt=tokens,\n",
        "    rng=jax.random.key(3),\n",
        "    max_sampling_steps=256,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJIL_1IxY5Cw"
      },
      "source": [
        "Transform the sampled output from named JAX array back to JAX array, and then\n",
        "decode it to text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJZKQe2d2uE_"
      },
      "outputs": [],
      "source": [
        "sample_tokens = samples.untag(\"batch\", \"seq\").unwrap()[0]\n",
        "tokenizer.decode(sample_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB--DuAiNgDu"
      },
      "source": [
        "## Loading a Sparse Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isXeCidj9w6t"
      },
      "source": [
        "After loading Gemma2 2B and showing the model could output reasonable text. Now,\n",
        "we load a sparse autoencoder (SAE).\n",
        "\n",
        "GemmaScope actually contains over four hundred SAEs, but for now we'll just load\n",
        "one on the residual stream at the end of layer 20 (of 26, note that layers start\n",
        "at 0 so this is the 21st layer. This is a fairly late layer, so the model should\n",
        "have time to find more abstract concepts!).\n",
        "\n",
        "The specific filename can be found at\n",
        "[google/gemma-scope-release](https://huggingface.co/collections/google/gemma-scope-release)  in `huggingface`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOM4R1__NfzF"
      },
      "outputs": [],
      "source": [
        "path_to_params = hf_hub_download(\n",
        "    repo_id=\"google/gemma-scope-2b-pt-res\",\n",
        "    filename=\"layer_20/width_16k/average_l0_71/params.npz\",\n",
        ")\n",
        "params = np.load(path_to_params, allow_pickle=True)\n",
        "params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4uRs57G94fY"
      },
      "source": [
        "Check the dimensions for SAE parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJJa5sVROCSQ"
      },
      "outputs": [],
      "source": [
        "sae_params = {k: v.shape for k, v in params.items()}\n",
        "sae_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSe7WfpiatIg"
      },
      "outputs": [],
      "source": [
        "np.linalg.norm(params[\"W_enc\"], axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_EeN2lVbD1l"
      },
      "source": [
        "## Implementing the SAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo25UB6uBWR6"
      },
      "source": [
        "We now define the forward pass of the SAE for pedagogical purposes using Penzai.\n",
        "\n",
        "Gemma Scope is a collection of\n",
        "[JumpReLU SAEs](https://arxiv.org/abs/2407.14435), which is like an auto-encoder\n",
        "with both encoder and decoder. The encoder is defined to map the activations\n",
        "into a sparse, non-negative vector of feature magnitude:\n",
        "\n",
        "$$\\boldsymbol{f}(\\boldsymbol{x})=\\sigma(\\boldsymbol{W}_{\\text{enc}}\\boldsymbol{x}+\\boldsymbol{b}_{\\text{enc}})$$\n",
        "\n",
        "Here $\\sigma$ is **JumpReLU** activation defined as ($H$ is the Heaviside step\n",
        "function and $\\theta$ is the threshold.)\n",
        "\n",
        "$$\\sigma(z)=zH(z-\\theta)$$\n",
        "\n",
        "Then the decoder reconstructs the input activations by:\n",
        "\n",
        "$$\\hat{\\boldsymbol{x}}=\\boldsymbol{W}_{\\text{dec}}\\boldsymbol{f}+\\boldsymbol{b}_{\\text{dec}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H1iL6O7dde5"
      },
      "source": [
        "As Penzai has not implemented such a JumpReLU auto-encoder, we first implement a\n",
        "class of `AutoEncoder` with properties of `encoder` and `decoder`. The model\n",
        "forward also includes `encode()` and `decode()`. Then we implement a class of\n",
        "`JumpReLU` with learnable parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kO7g9_mBVuA"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "from penzai.core import named_axes\n",
        "from penzai.core import struct\n",
        "from penzai.nn import layer as layer_base\n",
        "from penzai.nn import parameters\n",
        "from penzai.nn.linear_and_affine import LinearOperatorWeightInitializer\n",
        "from penzai.nn.linear_and_affine import zero_initializer\n",
        "\n",
        "\n",
        "NamedArray = named_axes.NamedArray\n",
        "\n",
        "\n",
        "@struct.pytree_dataclass\n",
        "class AutoEncoder(pz.nn.Layer):\n",
        "  \"\"\"Top-level auto-encoder wrapper.\n",
        "\n",
        "  Attributes:\n",
        "    encoder: The encoder to transform inputs to latents.\n",
        "    decoder: The decoder to reconstruct inputs from latents.\n",
        "  \"\"\"\n",
        "\n",
        "  encoder: pz.nn.Layer\n",
        "  decoder: pz.nn.Layer\n",
        "\n",
        "  def __call__(\n",
        "      self, x: named_axes.NamedArray, **side_inputs: Any\n",
        "  ) -> named_axes.NamedArray:\n",
        "    \"\"\"Applies the forward pass of the auto-encoder.\"\"\"\n",
        "    acts = self.encode(x, **side_inputs)\n",
        "    recon = self.decode(acts, **side_inputs)\n",
        "    return recon\n",
        "\n",
        "  def encode(\n",
        "      self, x: named_axes.NamedArray, **side_inputs: Any\n",
        "  ) -> named_axes.NamedArray:\n",
        "    \"\"\"Applies the encoder sublayer.\"\"\"\n",
        "    return self.encoder(x, **side_inputs)\n",
        "\n",
        "  def decode(\n",
        "      self, acts: named_axes.NamedArray, **side_inputs: Any\n",
        "  ) -> named_axes.NamedArray:\n",
        "    \"\"\"Applies the decoder sublayer.\"\"\"\n",
        "    return self.decoder(acts, **side_inputs)\n",
        "\n",
        "\n",
        "@struct.pytree_dataclass\n",
        "class JumpReLU(pz.nn.Layer):\n",
        "  \"\"\"JumpReLU activation.\"\"\"\n",
        "\n",
        "  threshold: parameters.ParameterLike[NamedArray]\n",
        "  new_axis_names: tuple[str, ...] = dataclasses.field(\n",
        "      metadata={\"pytree_node\": False}\n",
        "  )\n",
        "  act_fn: layer_base.Layer = pz.nn.Elementwise(jax.nn.relu)\n",
        "\n",
        "  def __call__(self, value: NamedArray, **_unused_side_inputs) -> NamedArray:\n",
        "    \"\"\"Return whether the value is above the threshold.\"\"\"\n",
        "    # Elementwise functions broadcast automatically\n",
        "    return (value > self.threshold.value) * self.act_fn(value)\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(\n",
        "      cls,\n",
        "      name: str,\n",
        "      init_base_rng: jax.Array | None,\n",
        "      threshold_axes: dict[str, int],\n",
        "      new_output_axes: dict[str, int] | None = None,\n",
        "      initializer: LinearOperatorWeightInitializer = zero_initializer,\n",
        "      dtype: jax.typing.DTypeLike = jnp.float32,\n",
        "  ):\n",
        "    \"\"\"Constructs an ``JumpReLU`` layer from a configuration.\n",
        "\n",
        "    Args:\n",
        "      name: The name of the layer.\n",
        "      init_base_rng: The base RNG to use for initializing model parameters.\n",
        "      threshold_axes: Names and lengths for the axes in the input that the\n",
        "        threshold should act over. Other axes will be broadcast over.\n",
        "      new_output_axes: Names and lengths of new axes that should be introduced\n",
        "        into the input.\n",
        "      initializer: Function to use to initialize the weight. Only the output\n",
        "        axes will be set.\n",
        "      dtype: Dtype for the threshold.\n",
        "\n",
        "    Returns:\n",
        "      A new ``AddThreshold`` layer with an uninitialized threshold parameter.\n",
        "    \"\"\"\n",
        "    if new_output_axes is None:\n",
        "      new_output_axes = {}\n",
        "\n",
        "    return cls(\n",
        "        threshold=parameters.make_parameter(\n",
        "            f\"{name}/threshold\",\n",
        "            init_base_rng,\n",
        "            initializer,\n",
        "            input_axes={},\n",
        "            output_axes={**threshold_axes, **new_output_axes},\n",
        "            parallel_axes={},\n",
        "            convolution_spatial_axes={},\n",
        "            dtype=dtype,\n",
        "        ),\n",
        "        new_axis_names=tuple(new_output_axes.keys()),\n",
        "    )\n",
        "\n",
        "  def treescope_color(self) -> str:\n",
        "    return \"#65cfbc\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5WwCTjQd4RW"
      },
      "source": [
        "After the definition of the above model layers, we implement the model\n",
        "definition of the whole SAE and bind it with parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pct9LAwSdaWn"
      },
      "outputs": [],
      "source": [
        "def sae_from_gemma_scope(\n",
        "    params_sae: dict[str, Any],\n",
        ") -> AutoEncoder:\n",
        "  \"\"\"Constructs an SAE model from Gemma scope parameters.\n",
        "\n",
        "  Args:\n",
        "    params_sae: The parameters of the Gemma scope.\n",
        "\n",
        "  Returns:\n",
        "    A new SAE model.\n",
        "  \"\"\"\n",
        "  embedding_dim, latents_dim = params_sae[\"W_enc\"].shape\n",
        "\n",
        "  # Encoder\n",
        "  encoder = pz.nn.Sequential([\n",
        "      pz.nn.Linear.from_config(\n",
        "          name=\"sae/W_enc\",\n",
        "          init_base_rng=None,\n",
        "          input_axes={\"embedding\": embedding_dim},\n",
        "          output_axes={\"latents\": latents_dim},\n",
        "      ),\n",
        "      pz.nn.AddBias.from_config(\n",
        "          name=\"sae/b_enc\",\n",
        "          init_base_rng=None,\n",
        "          biased_axes={\"latents\": latents_dim},\n",
        "      ),\n",
        "      JumpReLU.from_config(\n",
        "          name=\"sae\",\n",
        "          init_base_rng=None,\n",
        "          threshold_axes={\"latents\": latents_dim},\n",
        "      ),\n",
        "  ])\n",
        "  # Decoder\n",
        "  decoder = pz.nn.Sequential([\n",
        "      pz.nn.Linear.from_config(\n",
        "          name=\"sae/W_dec\",\n",
        "          init_base_rng=None,\n",
        "          input_axes={\"latents\": latents_dim},\n",
        "          output_axes={\"embedding\": embedding_dim},\n",
        "      ),\n",
        "      pz.nn.AddBias.from_config(\n",
        "          name=\"sae/b_dec\",\n",
        "          init_base_rng=None,\n",
        "          biased_axes={\"embedding\": embedding_dim},\n",
        "      ),\n",
        "  ])\n",
        "\n",
        "  # Create the model definition.\n",
        "  model_def = AutoEncoder(\n",
        "      encoder=encoder,\n",
        "      decoder=decoder,\n",
        "  )\n",
        "\n",
        "  # Create parameter objects for each parameter.\n",
        "  model_sae = pz.bind_variables(\n",
        "      model_def,\n",
        "      [\n",
        "          pz.Parameter(\n",
        "              value=pz.nx.wrap(params_sae[\"W_enc\"]).tag(\"embedding\", \"latents\"),\n",
        "              label=\"sae/W_enc.weights\",\n",
        "          ),\n",
        "          pz.Parameter(\n",
        "              value=pz.nx.wrap(params_sae[\"b_enc\"]).tag(\"latents\"),\n",
        "              label=\"sae/b_enc.bias\",\n",
        "          ),\n",
        "          pz.Parameter(\n",
        "              value=pz.nx.wrap(params_sae[\"W_dec\"]).tag(\"latents\", \"embedding\"),\n",
        "              label=\"sae/W_dec.weights\",\n",
        "          ),\n",
        "          pz.Parameter(\n",
        "              value=pz.nx.wrap(params_sae[\"b_dec\"]).tag(\"embedding\"),\n",
        "              label=\"sae/b_dec.bias\",\n",
        "          ),\n",
        "          pz.Parameter(\n",
        "              value=pz.nx.wrap(params_sae[\"threshold\"]).tag(\"latents\"),\n",
        "              label=\"sae/threshold\",\n",
        "          ),\n",
        "      ],\n",
        "  )\n",
        "  return model_sae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ2c46xLeKnj"
      },
      "source": [
        "By passing the params loaded from huggingface, we now get our SAE model. We can\n",
        "easily visualize the model structure in Penzai."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_p1FC2UNtLF"
      },
      "outputs": [],
      "source": [
        "sae_model = sae_from_gemma_scope(params)\n",
        "sae_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAVL2xN2eUp8"
      },
      "source": [
        "## Running the SAE on model activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhYotci1fmnl"
      },
      "source": [
        "In Penzai, it is easy to insert/delete/change model layers and manipulate\n",
        "activations. The general tutorial is in\n",
        "[Penzai Tutorials](https://penzai.readthedocs.io/en/stable/index.html). Here we\n",
        "only show how to display or save intermediate activations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xfN85I66gzH"
      },
      "outputs": [],
      "source": [
        "# Define a layer to visualize the middle activations\n",
        "@pz.pytree_dataclass  # <- This tags our class as being a Python dataclass and a JAX pytree node.\n",
        "class DisplayIntermediateValue(\n",
        "    pz.nn.Layer\n",
        "):  # <- pz.nn.Layer is the base class of Penzai layers.\n",
        "\n",
        "  def __call__(self, intermediate_value, **unused_side_inputs) -> Any:\n",
        "    # Show the value:\n",
        "    pz.show(\"Showing an intermediate value:\", intermediate_value)\n",
        "    # And return it unchanged.\n",
        "    return intermediate_value\n",
        "\n",
        "\n",
        "# Define a layer to extract the middle activations\n",
        "@pz.pytree_dataclass\n",
        "class SaveIntermediate(pz.nn.Layer):\n",
        "  saved: pz.StateVariable[Any | None]\n",
        "\n",
        "  def __call__(self, value: Any, /, **_unused_side_inputs) -> Any:\n",
        "    self.saved.value = value\n",
        "    return value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF27FoSDgzSK"
      },
      "source": [
        "Define a `StateVariable` to save model activations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw1Im0EOJilb"
      },
      "outputs": [],
      "source": [
        "destination = pz.StateVariable(value=None)\n",
        "destination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G66fLACQhBn2"
      },
      "source": [
        "In Penzai, model modifications are generally performed by using `pz.select` to\n",
        "make a modified copy of the original model (but sharing the same parameters).\n",
        "This involves “selecting” the part of the model you want to modify, then\n",
        "applying a modification, similar to the `.at[...].set(...)` syntax for modifying\n",
        "JAX arrays. Here we insert `SaveIntermediate` layer after 21 st\n",
        "`TransformerBlock`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKxPT-QTex40"
      },
      "outputs": [],
      "source": [
        "model_patched = (\n",
        "    pz.select(model)\n",
        "    .at_instances_of(penzai.models.transformer.model_parts.TransformerBlock)\n",
        "    .pick_nth_selected(20)\n",
        "    .insert_after(SaveIntermediate(destination))\n",
        ")\n",
        "logits = model_patched(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBvlBqvthbxY"
      },
      "source": [
        "Now we can visualize the activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1sYnIUjhfKN"
      },
      "outputs": [],
      "source": [
        "destination.value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DRWiK5Thimy"
      },
      "source": [
        "We can get some statistics from the activations. For example, we can visualize\n",
        "the $\\ell_2$-norm of residual streams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyTMsVfbhRf-"
      },
      "outputs": [],
      "source": [
        "# Check massive activations\n",
        "pz.nx.nmap(jnp.linalg.norm)(\n",
        "    destination.value.untag(\"embedding\"), ord=2, axis=-1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycqeuvOWh_kb"
      },
      "source": [
        "It is clear that the first token has large activations, which is called\n",
        "[massive activations](https://arxiv.org/abs/2402.17762)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlYdqjk9iJHJ"
      },
      "source": [
        "Now we can run SAE on the extracted activations. We first encode the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCg-O3o9XZ1U"
      },
      "outputs": [],
      "source": [
        "sae_acts = sae_model.encode(destination.value)\n",
        "sae_acts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w87DPrLLJQ7E"
      },
      "source": [
        "Here we can observe that except for the first token, other tokens have very\n",
        "sparse features in the latents. As the first token has outliers in activations\n",
        "(massive activations), it is not used for training SAEs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vPffb1hN2l1"
      },
      "source": [
        "Check the L0 of SAE, should be around 70."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daAXGvK1MzZI"
      },
      "outputs": [],
      "source": [
        "sparsity = pz.nx.nmap(jnp.sum)((sae_acts > 1).untag(\"latents\"), axis=-1)\n",
        "print(sparsity.untag(\"batch\", \"seq\").unwrap())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80MHNSezOC6l"
      },
      "source": [
        "Check the highest activating features on this input, on each token position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZLgTISHNYWX"
      },
      "outputs": [],
      "source": [
        "indices = pz.nx.nmap(jnp.argmax)(sae_acts.untag(\"latents\"), axis=-1)\n",
        "print(indices.untag(\"batch\", \"seq\").unwrap())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WnT1M5Kp8pc"
      },
      "source": [
        "So we see that one of the max activating examples on this question is\n",
        "[SAE feature 10004](https://www.neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/10004),\n",
        "which fires on concepts related to time travel!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ih-6zO3qBL4"
      },
      "source": [
        "## Steering Model Behaviors using SAEs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onwHwjNoqSRe"
      },
      "source": [
        "SAEs can be used to steer model behaviors. Here we reproduce one example in\n",
        "[steering](https://www.neuronpedia.org/api-doc#tag/steering/POST/api/steer)\n",
        "using Penzai."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8pbn-fyqfVO"
      },
      "source": [
        "Firstly, we prepare the prompt as a named JAX array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPKy_jnihd_7"
      },
      "outputs": [],
      "source": [
        "prompt = tokenizer.encode(\"The most iconic structure on Earth is\", add_bos=True)\n",
        "prompt = jnp.asarray(prompt)[None, :]\n",
        "tokens = pz.nx.wrap(prompt).tag(\"batch\", \"seq\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akpS1R2hs0wm"
      },
      "source": [
        "Then we run the baseline model forward:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV4FwdRPs33h"
      },
      "outputs": [],
      "source": [
        "inference_model = (\n",
        "    transformer.sampling_mode.KVCachingTransformerLM.from_uncached(\n",
        "        model,\n",
        "        cache_len=1024,\n",
        "        batch_axes={\"batch\": 1},\n",
        "    )\n",
        ")\n",
        "\n",
        "samples = transformer.simple_decoding_loop.temperature_sample_pyloop(\n",
        "    (\n",
        "        pz.select(inference_model)\n",
        "        .at(lambda root: root.body)\n",
        "        .apply(jit_wrapper.Jitted)\n",
        "    ),\n",
        "    prompt=tokens,\n",
        "    rng=jax.random.key(3),\n",
        "    temperature=0.5,\n",
        "    max_sampling_steps=64,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dygkCHA7s5iT"
      },
      "outputs": [],
      "source": [
        "sample_tokens = samples.untag(\"batch\", \"seq\").unwrap()[0]\n",
        "tokenizer.decode(sample_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW6I_15-s9rB"
      },
      "source": [
        "As shown in the decoding results, the model outputs \"the Great Pyramid of Giza\"\n",
        "and related description."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnkxY3wHqo1K"
      },
      "source": [
        "Here we would like the model to output references to SF. According to\n",
        "[neuronpedia](ttps://www.neuronpedia.org), we could identify that the index of\n",
        "latents which corresponding to this behavior. Then to amplify such model\n",
        "behavior, we can add a steering vector to the model activations. This steering\n",
        "vector is located in the same index of decoder matrix. The steer scale is an\n",
        "empirical value, one may need to obtain from experimental results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPBrz-22qEOh"
      },
      "outputs": [],
      "source": [
        "# steer references to SF\n",
        "steer_index = 3124  # reproduce this: https://www.neuronpedia.org/api-doc#tag/steering/POST/api/steer\n",
        "steer_scale = 38.5 * 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw7VmWQZqoCX"
      },
      "source": [
        "Remember that it is easy to modify model in Penzai. We can add a new model layer\n",
        "named as `SteerIntermediate` which adds steer vector to activations. We firstly\n",
        "take the parameters from SAE decoder, and then create an object of\n",
        "`SteerIntermediate` layer. We can use `.at[...].set(...)` syntax to insert the\n",
        "steering vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfo7ZpwJMfR_"
      },
      "outputs": [],
      "source": [
        "# Define a layer to steer the middle activations\n",
        "@pz.pytree_dataclass\n",
        "class SteerIntermediate(pz.nn.Layer):\n",
        "  steer_vector: pz.StateVariable\n",
        "  steer_scale: float\n",
        "\n",
        "  def __call__(self, value: Any, /, **_unused_side_inputs) -> Any:\n",
        "    steer_value = value + self.steer_vector * self.steer_scale\n",
        "    return steer_value\n",
        "\n",
        "\n",
        "steer_vector = (\n",
        "    pz.nx.wrap(params[\"W_dec\"][steer_index, :])\n",
        "    .tag(\"embedding\")\n",
        "    .astype(jnp.bfloat16)\n",
        ")\n",
        "\n",
        "model_patched = (\n",
        "    pz.select(model)\n",
        "    .at_instances_of(penzai.models.transformer.model_parts.TransformerBlock)\n",
        "    .pick_nth_selected(20)\n",
        "    .insert_after(SteerIntermediate(steer_vector, steer_scale))\n",
        ")\n",
        "steer_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq9QmVVtmFZo"
      },
      "outputs": [],
      "source": [
        "inference_model_patched = (\n",
        "    transformer.sampling_mode.KVCachingTransformerLM.from_uncached(\n",
        "        model_patched,\n",
        "        cache_len=1024,\n",
        "        batch_axes={\"batch\": 1},\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37y262T_mySw"
      },
      "outputs": [],
      "source": [
        "samples_patched = transformer.simple_decoding_loop.temperature_sample_pyloop(\n",
        "    (\n",
        "        pz.select(inference_model_patched)\n",
        "        .at(lambda root: root.body)\n",
        "        .apply(jit_wrapper.Jitted)\n",
        "    ),\n",
        "    prompt=tokens,\n",
        "    rng=jax.random.key(3),\n",
        "    temperature=0.5,\n",
        "    max_sampling_steps=64,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGOe868nnqoe"
      },
      "outputs": [],
      "source": [
        "sample_tokens = samples_patched.untag(\"batch\", \"seq\").unwrap()[0]\n",
        "tokenizer.decode(sample_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp0Y0EBhtJ5d"
      },
      "source": [
        "It is observed that with model steering, the model outputs \"the Golden Gate\n",
        "Bridge\", \"San Francisco\", \"Bay Area\", etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gfpln501qOAf"
      },
      "source": [
        "## Visualizing SAE features using Neuropedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfJVY1qpthr8"
      },
      "source": [
        "[neuronpedia](https://neuronpedia.org) provides nice visualization for SAE\n",
        "features. These visualization can be also loaded in the colab, which better\n",
        "interacts with Penzai. We only show an example as below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R2wGZ87NxzL"
      },
      "outputs": [],
      "source": [
        "from IPython.display import IFrame\n",
        "\n",
        "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
        "\n",
        "\n",
        "def get_dashboard_html(\n",
        "    sae_release=\"gemma-2-2b\", sae_id=\"20-gemmascope-res-16k\", feature_idx=0\n",
        "):\n",
        "  return html_template.format(sae_release, sae_id, feature_idx)\n",
        "\n",
        "\n",
        "html = get_dashboard_html(\n",
        "    sae_release=\"gemma-2-2b\", sae_id=\"20-gemmascope-res-16k\", feature_idx=10004\n",
        ")\n",
        "IFrame(html, width=1200, height=600)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [
        {
          "file_id": "1N8zaMQmhZnpnbomY3_UiD5my9HXddv_N",
          "timestamp": 1768145768504
        }
      ],
      "toc_visible": true,
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}