{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrGl1StDoE7h"
      },
      "source": [
        "# Testing Gemma3 SigLip in Penzai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlSt6SZVoM5j"
      },
      "source": [
        "This colab shows how to load and conduct model forward of SigLip in Gemma3\n",
        "series using our new package `gemma_penzai`. The original Penzai does not\n",
        "support vision transformers. The current version extends such support.\n",
        "\n",
        "NOTE: we run this colab on a TPU **v5e-1** runtime. Please see our notebook\n",
        "`./notebooks/gemma3_multimodal_penzai.ipynb` on how to build a local runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8EH5K21_JEi"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiHDcBlOUgC4"
      },
      "source": [
        "Firstly, we install `jax[tpu]`, `gemma_penzai` package and its dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLvMjeAP1h-j"
      },
      "outputs": [],
      "source": [
        "# Clone the gemma_penzai package\n",
        "!git clone https://github.com/google-deepmind/gemma_penzai.git\n",
        "\n",
        "# Upgrade your pip in case\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# Installs JAX with TPU support\n",
        "!pip install -U \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "\n",
        "# Install the package in editable mode (-e)\n",
        "# This installs dependencies defined in your pyproject.toml\n",
        "print(\"Installing gemma_penzai and dependencies...\")\n",
        "%cd gemma_penzai\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw0Y6x8lUiUo"
      },
      "source": [
        "Import miscellaneous packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtG6f0mu-9ZE"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "import kagglehub\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mguPfEtUm_B"
      },
      "source": [
        "Import JAX related packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vykG0cnfUmAr"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import Mesh\n",
        "from jax.sharding import NamedSharding\n",
        "from jax.sharding import PartitionSpec\n",
        "import orbax.checkpoint\n",
        "\n",
        "# check whether connects to TPU\n",
        "jax.devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DAL5CaUUpKh"
      },
      "source": [
        "Import `penzai` related packages (NOTE: we use the most up-to-dated version)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t807Q8sy50q9"
      },
      "outputs": [],
      "source": [
        "from penzai import pz\n",
        "import treescope\n",
        "\n",
        "treescope.basic_interactive_setup(autovisualize_arrays=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyfkEPhg52Pu"
      },
      "source": [
        "Import `gemma_penzai` package to use Gemma3 models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kse9LX_2_BQv"
      },
      "outputs": [],
      "source": [
        "from gemma_penzai import vision\n",
        "\n",
        "process_images = vision.image_utils.process_images\n",
        "gemma_vision_from_pretrained_checkpoint = (\n",
        "    vision.siglip.gemma_vision_from_pretrained_checkpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOs7O3h8_b2c"
      },
      "source": [
        "## Loading Gemma3 models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBF8VAEFVvDQ"
      },
      "source": [
        "### Load and shard model parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1k4NgDt_k6W"
      },
      "source": [
        "You can download the Gemma checkpoints using a Kaggle account and an API key. If\n",
        "you don't have an API key already, you can:\n",
        "\n",
        "1.  Visit https://www.kaggle.com/ and create an account if needed.\n",
        "\n",
        "2.  Go to your account settings, then the 'API' section.\n",
        "\n",
        "3.  Click 'Create new token' to download your key.\n",
        "\n",
        "Next, input your \"KAGGLE_USERNAME\" and \"KAGGLE_KEY\" below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lc1i-qVFsSFY"
      },
      "outputs": [],
      "source": [
        "KAGGLE_USERNAME = \"<KAGGLE_USERNAME>\"\n",
        "KAGGLE_KEY = \"<KAGGLE_KEY>\"\n",
        "try:\n",
        "  kagglehub.config.set_kaggle_credentials(KAGGLE_USERNAME, KAGGLE_KEY)\n",
        "except ImportError:\n",
        "  kagglehub.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMI36Snk1-W-"
      },
      "source": [
        "We load Gemma3-4B instruction model. The checkpoint path could be found in\n",
        "[Gemma's Documentation](https://gemma-llm.readthedocs.io/en/latest/checkpoints.html).\n",
        "Please note that only Gemma3 4B / 12B / 27B have the vision module, which is the\n",
        "same across different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05NGDgrysXEX"
      },
      "outputs": [],
      "source": [
        "weights_dir = kagglehub.model_download(\"google/gemma-3/flax/gemma3-4b-it\")\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6Dr_Xq3_HZm"
      },
      "outputs": [],
      "source": [
        "ckpt_path = os.path.join(weights_dir, \"gemma3-4b-it\")\n",
        "checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
        "metadata = checkpointer.metadata(ckpt_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S9DbrIUUt2U"
      },
      "source": [
        "Prepare the devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa5WLKYsUtfu"
      },
      "outputs": [],
      "source": [
        "n_devices = jax.local_device_count()\n",
        "sharding_devices = mesh_utils.create_device_mesh((n_devices,))\n",
        "mesh = Mesh(sharding_devices, (\"data\",))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXzuX4GlAYD3"
      },
      "source": [
        "Sharding the model parameters. The following sharding strategy splits model\n",
        "parameters into TPUs according to the last dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj2_r7SdAnyT"
      },
      "outputs": [],
      "source": [
        "restore_args = jax.tree_util.tree_map(\n",
        "    lambda m: orbax.checkpoint.ArrayRestoreArgs(\n",
        "        restore_type=jax.Array,\n",
        "        sharding=NamedSharding(\n",
        "            mesh, PartitionSpec(*(None,) * (len(m.shape) - 1), \"data\")\n",
        "        ),\n",
        "    ),\n",
        "    metadata.item_metadata,  # change back to metadata if any running error\n",
        ")\n",
        "flat_params = checkpointer.restore(ckpt_path, restore_args=restore_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyypEzTEVxuk"
      },
      "source": [
        "### Bind with Penzai model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em3UN1h9G4RC"
      },
      "source": [
        "Now we prepare the SigLip model definition and bind it with the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cDSo1EkYhKI"
      },
      "outputs": [],
      "source": [
        "vision_model = gemma_vision_from_pretrained_checkpoint(\n",
        "    flat_params,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9-VufNHWTGh"
      },
      "source": [
        "### Model visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YJ7XCZamDnv"
      },
      "source": [
        "Directly visualizing the model definition with parameters will take a long time.\n",
        "Therefore, we firstly use `unbind_params` function to extract the model\n",
        "architecture. Then we only visualize the model architecture without parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFyIIikkmDVZ"
      },
      "outputs": [],
      "source": [
        "model_unbound, _ = pz.unbind_params(vision_model)\n",
        "model_unbound"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaGND1MTHlvm"
      },
      "source": [
        "Now from the above visualization. We know the model class is\n",
        "`SigLipFromPatches`, and it has `siglip_encoder` and `siglip_exit` two model\n",
        "parts. `siglip_encoder` is an object of class `VisionTransformer`, while\n",
        "`siglip_exit` is a sequence of model layers, which is used for downstream tasks.\n",
        "In this case, `siglip_exit` downsamples the image tokens and also stop the\n",
        "gradient (as we want to freeze parameters of the vision module)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceCcN078Aqfw"
      },
      "source": [
        "Free some memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HDhsmS9A3_1"
      },
      "outputs": [],
      "source": [
        "del flat_params\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C96YC18Wo7G"
      },
      "source": [
        "## Test the model input/output for Gemma3 vision module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1irAGzCJWVxH"
      },
      "source": [
        "### Prepare the inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1G7u0BnI4Zg"
      },
      "source": [
        "First, let's load an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IRVrDmhYtZR"
      },
      "outputs": [],
      "source": [
        "ds = tfds.data_source(\"oxford_flowers102\", split=\"train\")\n",
        "image = ds[0][\"image\"]\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X8kYLUNI9mz"
      },
      "source": [
        "Then we visualize this image, it is a flower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWMJ5vKIng77"
      },
      "outputs": [],
      "source": [
        "plt.imshow(image)\n",
        "plt.axis(\"off\")  # Turn off axis labels\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59-NoWlyJCIq"
      },
      "source": [
        "Visualize the image shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSG7wXORJGZI"
      },
      "outputs": [],
      "source": [
        "image.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwzNx9PLJIRT"
      },
      "source": [
        "It is noted that the image could be any size. We need to process image before\n",
        "fed into the vision model. We provide `process_images` function to first resize\n",
        "the image and then patchify it. Please note that we have such input `[[image]]`\n",
        "to ensure the output has dimensions of `batch` (how many chat samples) and\n",
        "`frame` (how many images in each chat sample)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVyWpyfRY8n-"
      },
      "outputs": [],
      "source": [
        "images = process_images([[image]])\n",
        "images = pz.nx.wrap(images).tag(\"batch\", \"frame\", \"patch\", \"embedding\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhplybkMKTfL"
      },
      "source": [
        "Check the dimensions for model input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDdjnFFQdY-2"
      },
      "outputs": [],
      "source": [
        "images.named_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXGxb-tlZ-RQ"
      },
      "outputs": [],
      "source": [
        "images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rxb6CrpWbGY"
      },
      "source": [
        "### Model forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFJYNYAHgf14"
      },
      "source": [
        "Then we conduct the model forward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6s3dvDpUs4n"
      },
      "outputs": [],
      "source": [
        "out = vision_model(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BtDpJbagmXb"
      },
      "source": [
        "Visualize the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNq41KOUbPC9"
      },
      "outputs": [],
      "source": [
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSVTmPTLgnUa"
      },
      "source": [
        "As can be seen here, the image tokens are downsampled from 4k to 256. Then these\n",
        "image tokens are stitched with text tokens."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
