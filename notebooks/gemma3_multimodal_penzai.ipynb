{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrGl1StDoE7h"
      },
      "source": [
        "# Testing Gemma3 multimodal models in Penzai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlSt6SZVoM5j"
      },
      "source": [
        "This colab shows how to load and conduct model forward of Gemma3 multimodal\n",
        "models using our new package `gemma_penzai`. The original Penzai only supports\n",
        "Gemma1 and Gemma2 models. The current version extends supports for both vision\n",
        "models and vision language models, e.g., Gemma3.\n",
        "\n",
        "NOTE: we run this colab on a TPU **v5e-4** runtime and **v5e-1** runtime\n",
        "(default one) is not enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EObK5IojUJ8-"
      },
      "source": [
        "We provide a step-by-step tutorial on how to setup a TPU v5e-4 runtime on [Google Cloud Platform (GCP)](https://cloud.google.com/):\n",
        "1. Visit your [Google Cloud Platform (GCP)](https://cloud.google.com/) console and search for TPU, which is under \"Virtual machines\".\n",
        "2. Create a TPU node with TPU type \"v5litepod-4\" and TPU software version \"v2-alpha-tpuv5-lite\". Please set the TPU node name, description, and zone following your preferences.\n",
        "3. Open a terminal to connect your TPU node by running the command and forward the port (usually `8888`):\n",
        "    ```\n",
        "    gcloud compute tpus tpu-vm ssh <YOUR_TPU_NAME> \\\n",
        "    --zone=<YOUR_ZONE> \\\n",
        "    --project=<YOUR_PROJECT_ID> \\\n",
        "    -- -L 8888:localhost:8888\n",
        "    ```\n",
        "\n",
        "Afterwards, it is recommended to create a virtual environment to install `jupyter`. By default, Python version is 3.10, but we need at least Python 3.12 version. Follow the below steps:\n",
        "\n",
        "1. Install Python 3.12 on your TPU node by running the command:\n",
        "    ```\n",
        "    sudo apt-get update\n",
        "    sudo add-apt-repository ppa:deadsnakes/ppa\n",
        "    sudo apt-get update\n",
        "    sudo apt-get install python3.12 python3.12-venv python3.12-dev\n",
        "    ```\n",
        "2. Create your virtual environment:\n",
        "    ```\n",
        "    python3.12 -m venv <YOUR_VENV_NAME>\n",
        "    source <YOUR_VENV_NAME>/bin/activate\n",
        "    ```\n",
        "3. Install and start `Jupyter`:\n",
        "    ```\n",
        "    pip install jupyterlab\n",
        "    jupyter lab --port=8888 --no-browser\n",
        "    ```\n",
        "4. Look at the output in your terminal. Copy paste a URL that looks like this: `http://localhost:8888/lab?token=abc123...` to the Colab `Connect -> Connect to a local runtime`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8EH5K21_JEi"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WDRdob6RBmz"
      },
      "source": [
        "Firstly, we install `jax[tpu]`, `gemma_penzai` package and its dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNRLL8aJoEME"
      },
      "outputs": [],
      "source": [
        "# Clone the gemma_penzai package\n",
        "!git clone https://github.com/google-deepmind/gemma_penzai.git\n",
        "\n",
        "# Upgrade your pip in case\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# Installs JAX with TPU support\n",
        "!pip install -U \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "\n",
        "# Install the package in editable mode (-e)\n",
        "# This installs dependencies defined in your pyproject.toml\n",
        "print(\"Installing gemma_penzai and dependencies...\")\n",
        "%cd gemma_penzai\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i722SmKRJ_p"
      },
      "source": [
        "Import miscellaneous packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtG6f0mu-9ZE"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "from gemma import gm\n",
        "from IPython.display import clear_output\n",
        "import kagglehub\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cuD8IlzRUn1"
      },
      "source": [
        "Import JAX related packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng5VdfsTRJeX"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "from jax.experimental import mesh_utils\n",
        "import jax.numpy as jnp\n",
        "from jax.sharding import Mesh\n",
        "from jax.sharding import NamedSharding\n",
        "from jax.sharding import PartitionSpec\n",
        "import orbax.checkpoint\n",
        "\n",
        "# check whether connects to TPU\n",
        "jax.devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdWUzwEgRVRu"
      },
      "source": [
        "Import `penzai` related packages (NOTE: we use the most up-to-dated version)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kse9LX_2_BQv"
      },
      "outputs": [],
      "source": [
        "from penzai import pz\n",
        "from penzai.toolshed import jit_wrapper\n",
        "import treescope\n",
        "\n",
        "treescope.basic_interactive_setup(autovisualize_arrays=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64uhn9i597H7"
      },
      "source": [
        "Import `gemma_penzai` package to use Gemma3 models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpLKEhij9y1p"
      },
      "outputs": [],
      "source": [
        "from gemma_penzai import mllm\n",
        "from gemma_penzai import vision\n",
        "\n",
        "process_images = vision.image_utils.process_images\n",
        "gemma_multimodal_from_pretrained_checkpoint = (\n",
        "    mllm.load_gemma.gemma_multimodal_from_pretrained_checkpoint\n",
        ")\n",
        "sampling_mode = mllm.sampling_mode\n",
        "simple_decoding_loop = mllm.simple_decoding_loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFvZAV1-NwyE"
      },
      "source": [
        "By default, Jax do not utilize the full GPU memory, but this can be overwritten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SF-Dv6NOsbbR"
      },
      "outputs": [],
      "source": [
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOs7O3h8_b2c"
      },
      "source": [
        "## Loading Gemma3 multimodal model from Penzai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR9M242xP-rX"
      },
      "source": [
        "### Load and shard parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1k4NgDt_k6W"
      },
      "source": [
        "You can download the Gemma checkpoints using a Kaggle account and an API key. If\n",
        "you don't have an API key already, you can:\n",
        "\n",
        "1.  Visit https://www.kaggle.com/ and create an account if needed.\n",
        "\n",
        "2.  Go to your account settings, then the 'API' section.\n",
        "\n",
        "3.  Click 'Create new token' to download your key.\n",
        "\n",
        "Next, input your \"KAGGLE_USERNAME\" and \"KAGGLE_KEY\" below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJfPK500VJgq"
      },
      "outputs": [],
      "source": [
        "KAGGLE_USERNAME = \"<KAGGLE_USERNAME>\"\n",
        "KAGGLE_KEY = \"<KAGGLE_KEY>\"\n",
        "try:\n",
        "  kagglehub.config.set_kaggle_credentials(KAGGLE_USERNAME, KAGGLE_KEY)\n",
        "except ImportError:\n",
        "  kagglehub.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DQ0Nxsc0gZE"
      },
      "source": [
        "We load Gemma3-4B instruction model. The checkpoint path could be found in\n",
        "[Gemma's Documentation](https://gemma-llm.readthedocs.io/en/latest/checkpoints.html).\n",
        "Please note that only Gemma3 4B / 12B / 27B have the vision module, which is the\n",
        "same across different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FBssxv6VLaB"
      },
      "outputs": [],
      "source": [
        "weights_dir = kagglehub.model_download(\"google/gemma-3/flax/gemma3-4b-it\")\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6Dr_Xq3_HZm"
      },
      "outputs": [],
      "source": [
        "ckpt_path = os.path.join(weights_dir, \"gemma3-4b-it\")\n",
        "checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
        "metadata = checkpointer.metadata(ckpt_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnoZZSaBPypJ"
      },
      "source": [
        "We prepare the sharding devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "490m0RaiPyDn"
      },
      "outputs": [],
      "source": [
        "n_devices = jax.local_device_count()\n",
        "sharding_devices = mesh_utils.create_device_mesh((n_devices,))\n",
        "mesh = Mesh(sharding_devices, (\"data\",))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXzuX4GlAYD3"
      },
      "source": [
        "As multimodal design may require large memory usage, we may need more TPUs\n",
        "comparing to normal setup (in this case, we use 4x2 TPU v3). Therefore, we\n",
        "adopt an advanced sharding strategy by splitting model parameters according to\n",
        "the dimension which could be divided by the number of TPUs. The strategy is\n",
        "defined as the following function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHIySAHPoFmr"
      },
      "outputs": [],
      "source": [
        "def get_flexible_sharding(\n",
        "    array_metadata,\n",
        "    num_devices: int,\n",
        ") -> NamedSharding:\n",
        "  \"\"\"Determines the sharding for an array based on divisibility by num_devices, starting from the last dimension and shifting left if not divisible.\n",
        "\n",
        "  Args:\n",
        "      array_metadata: An object with 'shape' attribute (e.g., ArrayMetadata from\n",
        "        Orbax).\n",
        "      num_devices: The number of devices (e.g., TPUs).\n",
        "\n",
        "  Returns:\n",
        "      A NamedSharding object for the array.\n",
        "  \"\"\"\n",
        "  shape = array_metadata.shape\n",
        "  num_dims = len(shape)\n",
        "\n",
        "  if num_dims == 0:  # Scalar, no sharding needed\n",
        "    return NamedSharding(mesh, PartitionSpec())\n",
        "\n",
        "  # Iterate from the last dimension backwards\n",
        "  for i in range(num_dims - 1, -1, -1):\n",
        "    if shape[i] % num_devices == 0:\n",
        "      # If divisible, shard on this dimension\n",
        "      sharding_spec = [None] * num_dims\n",
        "      sharding_spec[i] = \"data\"\n",
        "      return NamedSharding(mesh, PartitionSpec(*sharding_spec))\n",
        "\n",
        "  # If no dimension is divisible, fall back to sharding the last dimension\n",
        "  # (or the first non-singleton if all are singletons, or just don't shard)\n",
        "  # For now, we'll default to the original strategy if no dimension is perfectly divisible\n",
        "  # You might want a different fallback depending on your specific needs (e.g., replicate)\n",
        "  print(\n",
        "      f\"No perfectly divisible dimension found for shape {shape}. Sharding last\"\n",
        "      \" dimension anyway.\"\n",
        "  )\n",
        "  return NamedSharding(mesh, PartitionSpec(*(None,) * (num_dims - 1), \"data\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQKAQJjNQFkN"
      },
      "source": [
        "Use our defined function `get_flexible_sharding` to shard the model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_wHhAIHQGZy"
      },
      "outputs": [],
      "source": [
        "restore_args = jax.tree_util.tree_map(\n",
        "    lambda m: orbax.checkpoint.ArrayRestoreArgs(\n",
        "        restore_type=jax.Array,\n",
        "        sharding=get_flexible_sharding(m, n_devices),\n",
        "    ),  # Only apply to Array Metadata\n",
        "    metadata.item_metadata,\n",
        ")\n",
        "flat_params = checkpointer.restore(ckpt_path, restore_args=restore_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAzQMRPETj8A"
      },
      "source": [
        "### Bind with Penzai model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNTAdEqEAc25"
      },
      "source": [
        "Now we prepare the Gemma3 multimodal language model definition in Penzai and\n",
        "bind it with the sharded parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M69PQ4STAqiM"
      },
      "outputs": [],
      "source": [
        "model = gemma_multimodal_from_pretrained_checkpoint(\n",
        "    flat_params,\n",
        "    upcast_activations_to_float32=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUyER7pCQT9R"
      },
      "source": [
        "### Model visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdKZZ_XrP2VX"
      },
      "source": [
        "Directly visualizing the model definition with parameters will take a long time.\n",
        "Therefore, we firstly use `unbind_params` function to extract the model\n",
        "architecture. Then we only visualize the model architecture without parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2bPV-wOgo3R"
      },
      "outputs": [],
      "source": [
        "model_unbound, _ = pz.unbind_params(model)\n",
        "model_unbound"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDtlM6xYP3Zb"
      },
      "source": [
        "Now from the above visualization. We know the model class is\n",
        "`MultiModalTransformerLM`, and it has `vision_transformer`, `vision_projection`,\n",
        "and `body` three model parts.\n",
        "\n",
        "`vision_transformer` is an object of class `SigLipFromPatches`,\n",
        "`vision_projection` projects image tokens into the space of text embeddings,\n",
        "while `body` refers to the main body of language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceCcN078Aqfw"
      },
      "source": [
        "Free some memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HDhsmS9A3_1"
      },
      "outputs": [],
      "source": [
        "del flat_params\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C96YC18Wo7G"
      },
      "source": [
        "## Evaluate the inference of Gemma3 models in Penzai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVSVmO0OQrKW"
      },
      "source": [
        "### Prepare the inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDSUhDQ7BxSu"
      },
      "source": [
        "Load tokenizer for Gemma3 models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMmuv6mgBwZo"
      },
      "outputs": [],
      "source": [
        "tokenizer = (\n",
        "    gm.text.Gemma3Tokenizer()\n",
        ")  # use gm.text.Gemma2Tokenizer() for Gemma 2 models.\n",
        "\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx8BlKnKRb0X"
      },
      "outputs": [],
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaSLAsI2R6Ch"
      },
      "outputs": [],
      "source": [
        "tokenizer.special_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf3uM8-2QmNv"
      },
      "source": [
        "To prepare the prompt, we need the instruction mode. And we need to add\n",
        "<start_of_image> special tokens where the images should be inserted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4myg_uWq7Bs"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"<start_of_turn>user\n",
        "What can you say about this image:\n",
        "\n",
        "<start_of_image>\n",
        "\n",
        "<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzXTqr6KCNu3"
      },
      "outputs": [],
      "source": [
        "prompt = tokenizer.encode(prompt, add_bos=True)\n",
        "prompt = jnp.asarray(prompt)[None, :]\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFn_F7jRdA9X"
      },
      "outputs": [],
      "source": [
        "tokens = pz.nx.wrap(prompt).tag(\"batch\", \"seq\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OwdDnJBtxsv"
      },
      "outputs": [],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTlXIz3TQy17"
      },
      "source": [
        "Then we load an image of flower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IRVrDmhYtZR"
      },
      "outputs": [],
      "source": [
        "ds = tfds.data_source(\"oxford_flowers102\", split=\"train\")\n",
        "image = ds[0][\"image\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK34VeQjQ367"
      },
      "source": [
        "Visualize the image using pyplot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfA0HQ4vbGa8"
      },
      "outputs": [],
      "source": [
        "plt.imshow(image)\n",
        "plt.axis(\"off\")  # Turn off axis labels\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYqGmMPQQ7AG"
      },
      "source": [
        "It is noted that the image could be any size. We need to process image before\n",
        "fed into the vision model. We provide `process_images` function to first resize\n",
        "the image and then patchify it. Please note that we have such input `[[image]]`\n",
        "to ensure the output has dimensions of `batch` (how many chat samples) and\n",
        "`frame` (how many images in each chat sample)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVyWpyfRY8n-"
      },
      "outputs": [],
      "source": [
        "images = process_images([[image]])\n",
        "images = pz.nx.wrap(images).tag(\"batch\", \"frame\", \"patch\", \"embedding\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDdjnFFQdY-2"
      },
      "outputs": [],
      "source": [
        "images.named_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pkpauHkRtaI"
      },
      "source": [
        "### Prepare the model with KV cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9hy7uu8RNoq"
      },
      "source": [
        "Similar to text-only model, before the inference, we prepare an inference mode\n",
        "by adding KV cache. We also need to pass the number of tokens for each image,\n",
        "can use `model.metadata`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4FwUl3vY1Tg"
      },
      "outputs": [],
      "source": [
        "inference_model = sampling_mode.KVCachingTransformerMultiModalLM.from_uncached(\n",
        "    model,\n",
        "    cache_len=1024,\n",
        "    batch_axes={\"batch\": 1},\n",
        "    num_tokens_per_image=model.metadata.num_tokens_per_image,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY9UIa32Rt8c"
      },
      "source": [
        "Then we jit the model and sample the output from the loop, same as text-only\n",
        "model. For demonstration, we employ a greedy decoding approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0iakSogRyLy"
      },
      "source": [
        "### Text generation based on multimodal inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOCoQxWac6n6"
      },
      "outputs": [],
      "source": [
        "samples = simple_decoding_loop.temperature_sample_pyloop(\n",
        "    (\n",
        "        pz.select(inference_model)\n",
        "        .at(lambda root: root.body)\n",
        "        .apply(jit_wrapper.Jitted)\n",
        "    ),\n",
        "    prompt=tokens,\n",
        "    images=images,\n",
        "    # temperature=1.0,\n",
        "    rng=jax.random.key(3),\n",
        "    max_sampling_steps=512,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2koJ3E_IR1Zz"
      },
      "source": [
        "Finally, we decode the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJZKQe2d2uE_"
      },
      "outputs": [],
      "source": [
        "sample_tokens = samples.untag(\"batch\", \"seq\").unwrap()[0]\n",
        "tokenizer.decode(sample_tokens)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
